{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lehuong240823/rag-company-knowledge-consultant-chatbot/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20GrbDZgi1mj"
      },
      "source": [
        "# Get Started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "QJQ8OZPL5war"
      },
      "outputs": [],
      "source": [
        "# @title Install Libraries\n",
        "!pip -q install pinecone llama-cpp-python\n",
        "!pip -q install langchain_community langchain_openai langchain_pinecone langchain-huggingface langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "nRwdJs_82XKX",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8da3ec99-bb29-4746-fc3e-fa1d153bb89e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/langchain_pinecone/__init__.py:3: LangChainDeprecationWarning: As of langchain-core 0.3.0, LangChain uses pydantic v2 internally. The langchain_core.pydantic_v1 module was a compatibility shim for pydantic v1, and should no longer be used. Please update the code to import from Pydantic directly.\n",
            "\n",
            "For example, replace imports like: `from langchain_core.pydantic_v1 import BaseModel`\n",
            "with: `from pydantic import BaseModel`\n",
            "or the v1 compatibility namespace if you are working in a code base that has not been fully upgraded to pydantic 2 yet. \tfrom pydantic.v1 import BaseModel\n",
            "\n",
            "  from langchain_pinecone.vectorstores import Pinecone, PineconeVectorStore\n"
          ]
        }
      ],
      "source": [
        "# @title Import Libraries\n",
        "import os, yaml, inspect, json, datetime, pytz, hashlib\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "from google.colab import userdata\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.chat_models import ChatLlamaCpp\n",
        "from langchain.document_loaders import TextLoader, CSVLoader, GithubFileLoader\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.embeddings import LlamaCppEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langsmith import Client, traceable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configuration\n",
        "config = yaml.safe_load('''\n",
        "repo: lehuong240823/rag-company-knowledge-consultant-chatbot\n",
        "branch: main\n",
        "ref_data: reference_data\n",
        "format:\n",
        "  - .txt\n",
        "  - .csv\n",
        "chat_model:\n",
        "  name: Qwen3-0.6B-Q8_0-GGUF\n",
        "  path: /content/Qwen3-0.6B-Q8_0.gguf\n",
        "  source: https://huggingface.co/Qwen/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q8_0.gguf\n",
        "embedding_model:\n",
        "  name: Qwen3-Embedding-0.6B-Q8_0-GGUF\n",
        "  path: /content/Qwen3-Embedding-0.6B-Q8_0.gguf\n",
        "  source: https://huggingface.co/Qwen/Qwen3-Embedding-0.6B-GGUF/resolve/main/Qwen3-Embedding-0.6B-Q8_0.gguf\n",
        "grader_model:\n",
        "  name: 'Llama-3.2-1B-Instruct-Q8_0-GGUF'\n",
        "  path: '/content/llama-3.2-1b-instruct-q8_0.gguf'\n",
        "  source: 'https://huggingface.co/hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF/resolve/main/llama-3.2-1b-instruct-q8_0.gguf'\n",
        "\n",
        "''')"
      ],
      "metadata": {
        "id": "utocHmVI51nB",
        "cellView": "form"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download Quantized Model\n",
        "!wget -N -q {config['chat_model']['source']}\n",
        "!wget -N -q {config['embedding_model']['source']}\n",
        "!wget -N -q {config['grader_model']['source']}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gGF99SqXsnpF",
        "cellView": "form"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "_SC-bUdQ15Aq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Set Environment Variables\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ['LANGSMITH_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGSMITH_PROJECT'] = userdata.get('PINECONE_INDEX_NAME')\n",
        "os.environ['LANGSMITH_TRACING'] = 'true'\n",
        "os.environ['OPENAI_API_BASE'] = 'https://router.huggingface.co/v1' #'https://openrouter.ai/api/v1'\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('HF_TOKEN') #userdata.get('OPENROUTER_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['PINECONE_INDEX_NAME'] = userdata.get('PINECONE_INDEX_NAME')\n",
        "os.environ['GITHUB_PERSONAL_ACCESS_TOKEN'] = userdata.get('GITHUB_PERSONAL_ACCESS_TOKEN')\n",
        "root_path =  userdata.get('ROOT_PATH')\n",
        "embedding_path = join(root_path, 'embedding_data')\n",
        "reference_path = join(root_path, 'reference_data')\n",
        "evaluators_path = join(root_path, 'evaluators')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJUrQT_gZ5Z3"
      },
      "source": [
        "# Method"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Utility Functions"
      ],
      "metadata": {
        "id": "V2oFYjlwHp1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "0RfuW_ILI5vx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Timestamp and Hashing Utilities\n",
        "def get_timestamp():\n",
        "  vietnam_timezone = pytz.timezone('Asia/Ho_Chi_Minh')\n",
        "  vietnam_time = datetime.datetime.now(vietnam_timezone)\n",
        "  return vietnam_time.isoformat()\n",
        "\n",
        "def md5(text):\n",
        "  '''MD% hash for track change'''\n",
        "  return hashlib.md5(text.encode('utf-8')).hexdigest()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "cellView": "form",
        "id": "EwutUmxfMdsj"
      },
      "outputs": [],
      "source": [
        "# @title Pinecone Utilities\n",
        "def get_index(index_name=os.environ['PINECONE_INDEX_NAME']):\n",
        "  pc = Pinecone()\n",
        "  if not pc.has_index(index_name):\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1024,\n",
        "        metric='cosine',\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region='us-east-1'\n",
        "        )\n",
        "    )\n",
        "\n",
        "  return pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "id": "WTe9PQ6I8fV4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Document Loading and Processing Utilities\n",
        "def create_text_splitter(chunk_size=100, chunk_overlap=0):\n",
        "  return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "def Loader(file=None, mode='github', repo=config['repo'], branch=config['branch'], dest_dir='reference_data', format=('.txt', '.csv')):\n",
        "  if file is None and mode=='github':\n",
        "    loader = GithubFileLoader(\n",
        "      repo=repo,\n",
        "      branch=branch,\n",
        "      file_filter=lambda file_path: (file_path.endswith(format) and file_path.startswith(dest_dir))\n",
        "    )\n",
        "\n",
        "  if file is not None and mode=='local':\n",
        "    if file.endswith('.csv'):\n",
        "      loader = CSVLoader(file)\n",
        "    elif file.endswith('.txt'):\n",
        "      loader = TextLoader(file)\n",
        "\n",
        "  return loader\n",
        "\n",
        "def add_texts(text):\n",
        "  texts = text_splitter.split_text(text)\n",
        "  vectorstore.add_texts(texts)\n",
        "  print(f'Added {len(texts)} text chunks to vector store.')\n",
        "\n",
        "def add_documents(path=None, ids=None, mode='github'):\n",
        "  loader = Loader(file=path, mode=mode)\n",
        "  documents = loader.load()\n",
        "  src = '\\n'.join([doc.metadata['path'] for doc in documents]) if path is None else path\n",
        "\n",
        "  print(f'Loaded {len(documents)} document(s) from: {src}')\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "\n",
        "  for idx, each in enumerate(docs):\n",
        "    part = 0 if idx==0 or each.metadata['source']!=docs[idx-1].metadata['source'] else part + 1\n",
        "    each.metadata['timestamp'] = get_timestamp()\n",
        "    each.metadata['md5'] = md5(each.page_content)\n",
        "    if ids is None:\n",
        "      id = each.metadata['path'].split('/')[-1].split('.')[0] + f'_{part}'\n",
        "    else:\n",
        "      id = f'{ids}_{str(idx)}'\n",
        "    vectorstore.add_documents([each], ids=[id])\n",
        "  #vectorstore.add_documents(documents=docs)\n",
        "  print(f'Added {len(docs)} text chunk(s) to vector store.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "cellView": "form",
        "id": "SBX_Qx0pMcuq"
      },
      "outputs": [],
      "source": [
        "# @title LangSmith Dataset Utilities\n",
        "def create_dataset(file_name=None, dataset_name=None, provider='langsmith', mode='local', overwrite=False):\n",
        "  '''Load json file only'''\n",
        "  if mode=='local' and file_name!=None:\n",
        "    dataset_name = file_name[:-5] if dataset_name is None else dataset_name\n",
        "    path = join(embedding_path, provider, file_name)\n",
        "    with open(path) as f:\n",
        "      if not client.has_dataset(dataset_name=dataset_name):\n",
        "        client.create_dataset(dataset_name=dataset_name)\n",
        "        client.create_examples(\n",
        "          dataset_name=dataset_name,\n",
        "          examples=json.load(f)\n",
        "        )\n",
        "        print(f'Create {dataset_name} successfully from {path}!')\n",
        "      else:\n",
        "        print(f'Dataset {dataset_name} already exists.')\n",
        "\n",
        "  if mode=='github':\n",
        "    json_loader = Loader(mode='github', dest_dir=f'embedding_data/{provider}', format=('json'))\n",
        "    jsons = json_loader.load()\n",
        "    for each in jsons:\n",
        "      dataset_name = each.metadata['path'].split('/')[-1].split('.')[0]\n",
        "      if overwrite:\n",
        "        client.delete_dataset(dataset_name=dataset_name)\n",
        "      if not client.has_dataset(dataset_name=dataset_name):\n",
        "        client.create_dataset(dataset_name=dataset_name)\n",
        "        client.create_examples(\n",
        "          dataset_name=dataset_name,\n",
        "          examples=json.loads(each.page_content)\n",
        "        )\n",
        "        print(f'Create {dataset_name} successfully from {each.metadata[\"source\"]}!')\n",
        "      else:\n",
        "        print(f'Dataset {dataset_name} already exists.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "EMr6Xf8SnsEt"
      },
      "outputs": [],
      "source": [
        "# @title Evaluator Utilities\n",
        "def get_evaluator_path(evaluator_name, mode='github'):\n",
        "  if mode=='local':\n",
        "    path = join(evaluators_path, evaluator_name)\n",
        "  if mode=='github':\n",
        "    path = join('evaluators', evaluator_name)\n",
        "  return path\n",
        "\n",
        "def load_chat_prompt(evaluator, mode='github'):\n",
        "  role_map = {'system': SystemMessagePromptTemplate, 'user': HumanMessagePromptTemplate, 'ai': AIMessagePromptTemplate}\n",
        "  messages = []\n",
        "  file_path = join(get_evaluator_path(evaluator, mode), 'prompt.yaml')\n",
        "  if mode=='local':\n",
        "    with open(file_path, 'r') as f:\n",
        "      config = yaml.safe_load(f)\n",
        "  if mode=='github':\n",
        "    yaml_loader = Loader(mode=mode, dest_dir=file_path, format=('.yaml')).load()\n",
        "    config = yaml.safe_load(yaml_loader[0].page_content)\n",
        "\n",
        "  for msg_config in config['messages']:\n",
        "    messages.append(role_map[msg_config['role']].from_template(msg_config['template']))\n",
        "\n",
        "  return ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "def load_output_schema(evaluator, mode='github'):\n",
        "  file_path = join(get_evaluator_path(evaluator, mode), 'output_schema.yaml')\n",
        "  if mode=='local':\n",
        "    with open(file_path, 'r') as f:\n",
        "      config = yaml.safe_load(f)\n",
        "  if mode=='github':\n",
        "    yaml_loader = Loader(mode=mode, dest_dir=file_path, format=('.yaml')).load()\n",
        "    config = yaml.safe_load(yaml_loader[0].page_content)\n",
        "  return config\n",
        "\n",
        "def create_structured_grader_llm(temperature=0, evaluator=None):\n",
        "  return ChatLlamaCpp(\n",
        "    temperature=0,\n",
        "    model_path='/content/llama-3.2-1b-instruct-q8_0.gguf',\n",
        "    n_ctx=2048,\n",
        "    n_batch=256,\n",
        "    max_tokens=200,\n",
        "    top_p=0.8,\n",
        "    top_k=20,\n",
        "    repeat_penalty=1.5,\n",
        "    verbose=False,\n",
        "    streaming=False,\n",
        "  ).with_structured_output(\n",
        "    load_output_schema(evaluator),\n",
        "  )\n",
        "  \"\"\"return ChatOpenAI(model=model, temperature=temperature).with_structured_output(\n",
        "    load_output_schema(evaluator),\n",
        "    method='json_schema', strict=True\n",
        "  )\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "275hVmB1dD74"
      },
      "source": [
        "## Evaluators"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Citeria"
      ],
      "metadata": {
        "id": "k61r41bTAaEX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "id": "vi1zoF_NfyRF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Correctness\n",
        "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    question=inputs['question'],\n",
        "    reference=reference_outputs['answer'],\n",
        "    answer=outputs['answer']\n",
        "  ))\n",
        "  return grade['correct']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "cellView": "form",
        "id": "LEHie9jeeQc9"
      },
      "outputs": [],
      "source": [
        "# @title Relevance\n",
        "def relevance(inputs: dict, outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    question=inputs['question'],\n",
        "    answer=outputs['answer']\n",
        "  ))\n",
        "  return grade['relevant']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "cellView": "form",
        "id": "KpkhJOawfBrC"
      },
      "outputs": [],
      "source": [
        "# @title Groundedness\n",
        "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    facts='\\n'.join(doc.page_content for doc in outputs['documents']),\n",
        "    answer=outputs['answer']\n",
        "  ))\n",
        "  return grade['grounded']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "cellView": "form",
        "id": "50XKGfJpf3ew"
      },
      "outputs": [],
      "source": [
        "# @title Retrieval relevance\n",
        "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    facts='\\n'.join(doc.page_content for doc in outputs['documents']),\n",
        "    question=inputs['question']\n",
        "  ))\n",
        "  return grade['relevant']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target and Experiment"
      ],
      "metadata": {
        "id": "SJME-zuwAkx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "cellView": "form",
        "id": "46aanRpmFDYw"
      },
      "outputs": [],
      "source": [
        "# @title Target\n",
        "@traceable()\n",
        "def rag_bot(question: str) -> dict:\n",
        "  ai_msg = qa_chain.invoke({'question': question, 'chat_history': []})\n",
        "  return {'answer': ai_msg['answer'], 'documents': ai_msg['source_documents']}\n",
        "\n",
        "def target(inputs: dict) -> dict:\n",
        "  return rag_bot(inputs['question'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "Iqy-K-Qtwp3R",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Experiment\n",
        "def experiment(chunk_size, overlap, k, experiment_prefix='rag-doc-relevance', data='fqa_mini'):\n",
        "  experiment_results = client.evaluate(\n",
        "    target,\n",
        "    data=data,\n",
        "    evaluators=[correctness, relevance, groundedness, retrieval_relevance],\n",
        "    experiment_prefix=experiment_prefix\n",
        "  )\n",
        "\n",
        "  return experiment_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning"
      ],
      "metadata": {
        "id": "nJ3LO6Jw9kGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dump Experiments to CSV\n",
        "def save_experiment(df, path=join(root_path, 'experiments', 'tuning_experiments')):\n",
        "  if not os.path.isfile(path):\n",
        "    return df.to_csv(path, index=False, mode='w', encoding='utf-8')\n",
        "  else:\n",
        "    return df.to_csv(path, index=False, mode='a', header=False, encoding='utf-8')\n",
        "  print(f'Save {path} successfully!')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jARHsIIwQjAc"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Chunk Size\n",
        "def chunk_size_tunning(start=200, end=500, step=100, chunk_overlap=0, k=2, sync_embedded_data=False):\n",
        "  for chunk_size in range(start, end+step, step):\n",
        "    namespace = f'chunk{chunk_size}_overlap{chunk_overlap}'\n",
        "    prefix = f'tune_{namespace}_k{k}'\n",
        "    print(f'Begin chunk tunning {prefix}')\n",
        "\n",
        "    init_global_var(namespace=namespace, chunk_size=chunk_size, chunk_overlap=chunk_overlap, k=k)\n",
        "\n",
        "    if sync_embedded_data:\n",
        "      add_documents(mode='github')\n",
        "\n",
        "    experiment_results = experiment(chunk_size=chunk_size, overlap=chunk_overlap, k=k, experiment_prefix=prefix)\n",
        "    pd_resutlts = experiment_results.to_pandas()\n",
        "    pd_resutlts['experiment'] = prefix\n",
        "    pd_resutlts['chunk'] = chunk_size\n",
        "    pd_resutlts['overlap'] = chunk_overlap\n",
        "    pd_resutlts['k'] = k\n",
        "\n",
        "    print(f'Complete chunk tunning {prefix}. TOTAL: {(chunk_size-start)/(end-start+step)*100}%')"
      ],
      "metadata": {
        "id": "77983-fL1y0j",
        "cellView": "form"
      },
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "collapsed": true,
        "id": "KCuTVFQOp-z_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Initialize Components\n",
        "def init_global_var(namespace=None, chunk_size=100, chunk_overlap=0, k=2, model='Qwen/Qwen3-8B:featherless-ai'):\n",
        "  global index, client, llm, embeddings, vectorstore, text_splitter, qa_chain\n",
        "\n",
        "  index = get_index()\n",
        "\n",
        "  client = Client()\n",
        "\n",
        "  #llm = ChatOpenAI(model=model)\n",
        "\n",
        "  llm = LlamaCpp(\n",
        "    model_path=config['chat_model']['path'],\n",
        "    temperature=0,\n",
        "    n_ctx=1024,\n",
        "    max_tokens=100,\n",
        "    n_batch=256,\n",
        "    top_p=0.8,\n",
        "    top_k=20,\n",
        "    repeat_penalty=1.5,\n",
        "    stop=[\"\\n\"],\n",
        "    streaming=False,\n",
        "    verbose=False,\n",
        "  )\n",
        "\n",
        "  '''embeddings = HuggingFaceEndpointEmbeddings(model='Qwen/Qwen3-Embedding-8B')'''\n",
        "\n",
        "  embeddings = LlamaCppEmbeddings(\n",
        "    model_path=config['embedding_model']['path'],\n",
        "    verbose=False\n",
        "  )\n",
        "\n",
        "  vectorstore = PineconeVectorStore(index=index, embedding=embeddings, namespace=namespace)\n",
        "\n",
        "  text_splitter = create_text_splitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "  retriever = vectorstore.as_retriever(k=k, search_kwargs={ \"namespace\": namespace })\n",
        "\n",
        "  qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    combine_docs_chain_kwargs={'prompt': load_chat_prompt('qa_chain')},\n",
        "    return_source_documents=True\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_global_var(namespace='chunk200_overlap0', )\n",
        "experiment_results = experiment(chunk_size=100, overlap=0, k=2, experiment_prefix='TEST')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356,
          "referenced_widgets": [
            "051672acd0ef42189c1373d66dc0063d",
            "4fff2b881f2f4f119571d088fc76ee50",
            "f72392fd4ebd46f38b5003c77f4a5cc0",
            "86030632b71041388f7620ee5707fba7",
            "9041ff193489421484c0c04c51a3af14",
            "ad12eac47a6e4cc2993772415d8f4d08",
            "d1ef69d301fc4c5daf2e505b214df0cd",
            "7aa7984e72f24e9dbe8a8c315f750e91",
            "8b242063058a45adb5e0987103fa64f7",
            "987f967cc11e4f3d961d8b0b0de6725d",
            "0796b431292c4ca28cfa292108daff9a"
          ]
        },
        "collapsed": true,
        "id": "XxG49-1Rflyk",
        "outputId": "ccb52d32-103e-411e-a21a-46112e6dade1"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (512) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'TEST-09012583' at:\n",
            "https://smith.langchain.com/o/00fa7863-e8c4-446a-9682-0dec08610f9c/datasets/acbc8249-ad4a-4ea9-b94d-49cbfe49edad/compare?selectedSessions=8fb9758c-8d9a-4d06-8f00-7aac11b611df\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "051672acd0ef42189c1373d66dc0063d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "create_dataset(mode='github', overwrite=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kPioEazL4aZG",
        "outputId": "3a61b70a-4a09-4769-f3b1-63f8843849fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Create fqa_20250911 successfully from https://api.github.com/lehuong240823/rag-company-knowledge-consultant-chatbot/blob/main/embedding_data/langsmith/fqa_20250911.json!\n",
            "Create fqa_mini successfully from https://api.github.com/lehuong240823/rag-company-knowledge-consultant-chatbot/blob/main/embedding_data/langsmith/fqa_mini.json!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsYi9z3UeX9N",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "chunk_size_tunning()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YgM_RtXZXIn"
      },
      "outputs": [],
      "source": [
        "print(experiment_results.to_pandas().to_markdown())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "20GrbDZgi1mj",
        "k61r41bTAaEX",
        "SJME-zuwAkx3"
      ],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "mount_file_id": "1UvmP-pkUnlIOrt37odrdShOdS3-RVI7_",
      "authorship_tag": "ABX9TyP88IW+Kyw6Ky37bsQinZUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "051672acd0ef42189c1373d66dc0063d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_4fff2b881f2f4f119571d088fc76ee50",
              "IPY_MODEL_f72392fd4ebd46f38b5003c77f4a5cc0",
              "IPY_MODEL_86030632b71041388f7620ee5707fba7"
            ],
            "layout": "IPY_MODEL_9041ff193489421484c0c04c51a3af14"
          }
        },
        "4fff2b881f2f4f119571d088fc76ee50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ad12eac47a6e4cc2993772415d8f4d08",
            "placeholder": "​",
            "style": "IPY_MODEL_d1ef69d301fc4c5daf2e505b214df0cd",
            "value": ""
          }
        },
        "f72392fd4ebd46f38b5003c77f4a5cc0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7aa7984e72f24e9dbe8a8c315f750e91",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8b242063058a45adb5e0987103fa64f7",
            "value": 1
          }
        },
        "86030632b71041388f7620ee5707fba7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_987f967cc11e4f3d961d8b0b0de6725d",
            "placeholder": "​",
            "style": "IPY_MODEL_0796b431292c4ca28cfa292108daff9a",
            "value": " 2/? [03:53&lt;00:00, 116.60s/it]"
          }
        },
        "9041ff193489421484c0c04c51a3af14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ad12eac47a6e4cc2993772415d8f4d08": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d1ef69d301fc4c5daf2e505b214df0cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7aa7984e72f24e9dbe8a8c315f750e91": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8b242063058a45adb5e0987103fa64f7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "987f967cc11e4f3d961d8b0b0de6725d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0796b431292c4ca28cfa292108daff9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}