{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lehuong240823/rag-company-knowledge-consultant-chatbot/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20GrbDZgi1mj"
      },
      "source": [
        "# Get Started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 119,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "QJQ8OZPL5war",
        "outputId": "9034a7a2-d707-48eb-9c04-8908d31f9da5",
        "cellView": "form"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1G\u001b[0K⠙\u001b[1G\u001b[0K⠹\u001b[1G\u001b[0K⠸\u001b[1G\u001b[0K⠼\u001b[1G\u001b[0K⠴\u001b[1G\u001b[0K⠦\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94mcode\u001b[39m E404\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m Not Found - GET https://registry.npmjs.org/streamlit - Not found\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m  'streamlit@*' is not in this registry.\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m Note that you can also install from a\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m \u001b[94m404\u001b[39m tarball, folder, http url, or git url.\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K\u001b[1mnpm\u001b[22m \u001b[31merror\u001b[39m A complete log of this run can be found in: /root/.npm/_logs/2025-09-25T05_36_49_095Z-debug-0.log\n",
            "\u001b[1G\u001b[0K⠧\u001b[1G\u001b[0K"
          ]
        }
      ],
      "source": [
        "# @title Install Libraries\n",
        "!pip -q install pinecone llama-cpp-python streamlit\n",
        "!pip -q install langchain_community langchain_openai langchain_pinecone langchain-huggingface langsmith\n",
        "!npm -q install localtunnel streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "form",
        "id": "nRwdJs_82XKX"
      },
      "outputs": [],
      "source": [
        "# @title Import Libraries\n",
        "import os, yaml, inspect, json, datetime, pytz, hashlib\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "from google.colab import userdata\n",
        "import streamlit as st\n",
        "from IPython.display import display, HTML\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.chat_models import ChatLlamaCpp\n",
        "from langchain.document_loaders import TextLoader, CSVLoader, GithubFileLoader\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.embeddings import LlamaCppEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langsmith import Client, traceable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "utocHmVI51nB"
      },
      "outputs": [],
      "source": [
        "# @title Configuration\n",
        "config = yaml.safe_load('''\n",
        "repo: lehuong240823/rag-company-knowledge-consultant-chatbot\n",
        "branch: main\n",
        "ref_data: reference_data\n",
        "subdomain: apchatbot\n",
        "format:\n",
        "  - .txt\n",
        "  - .csv\n",
        "chat_model:\n",
        "  name: Qwen3-0.6B-Q8_0-GGUF\n",
        "  path: /content/Qwen3-0.6B-Q8_0.gguf\n",
        "  alter_path: /kaggle/working/Qwen3-0.6B-Q8_0.gguf\n",
        "  source: https://huggingface.co/Qwen/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q8_0.gguf\n",
        "  params:\n",
        "    temperature: 0\n",
        "    n_ctx: 1024\n",
        "    n_batch: 256\n",
        "    suffix: Quý khách có cần hỗ trợ thêm gì không?\n",
        "    max_tokens: 100\n",
        "    top_p: 0.8\n",
        "    top_k: 20\n",
        "    repeat_penalty: 1.5\n",
        "embedding_model:\n",
        "  name: Qwen3-Embedding-0.6B-Q8_0-GGUF\n",
        "  path: /content/Qwen3-Embedding-0.6B-Q8_0.gguf\n",
        "  alter_path: /kaggle/working/Qwen3-Embedding-0.6B-Q8_0.gguf\n",
        "  source: https://huggingface.co/Qwen/Qwen3-Embedding-0.6B-GGUF/resolve/main/Qwen3-Embedding-0.6B-Q8_0.gguf\n",
        "  params:\n",
        "    n_ctx: 1024\n",
        "    n_batch: 256\n",
        "grader_model:\n",
        "  name: Llama-3.2-1B-Instruct-Q8_0-GGUF\n",
        "  path: /content/llama-3.2-1b-instruct-q8_0.gguf\n",
        "  alter_path: /kaggle/working/llama-3.2-1b-instruct-q8_0.gguf\n",
        "  source: https://huggingface.co/hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF/resolve/main/llama-3.2-1b-instruct-q8_0.gguf\n",
        "  params:\n",
        "    temperature: 0\n",
        "    n_ctx: 2048\n",
        "    n_batch: 256\n",
        "    max_tokens: 100\n",
        "    top_p: 0.8\n",
        "    top_k: 20\n",
        "    repeat_penalty: 1.5\n",
        "''')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "collapsed": true,
        "id": "gGF99SqXsnpF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Download Quantized Model\n",
        "!wget -N -q {config['chat_model']['source']}\n",
        "!wget -N -q {config['embedding_model']['source']}\n",
        "!wget -N -q {config['grader_model']['source']}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "form",
        "id": "_SC-bUdQ15Aq"
      },
      "outputs": [],
      "source": [
        "# @title Set Environment Variables\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ['LANGSMITH_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGSMITH_PROJECT'] = userdata.get('PINECONE_INDEX_NAME')\n",
        "os.environ['LANGSMITH_TRACING'] = 'true'\n",
        "os.environ['OPENAI_API_BASE'] = 'https://router.huggingface.co/v1' #'https://openrouter.ai/api/v1'\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('HF_TOKEN') #userdata.get('OPENROUTER_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['PINECONE_INDEX_NAME'] = userdata.get('PINECONE_INDEX_NAME')\n",
        "os.environ['GITHUB_PERSONAL_ACCESS_TOKEN'] = userdata.get('GITHUB_PERSONAL_ACCESS_TOKEN')\n",
        "root_path =  userdata.get('ROOT_PATH')\n",
        "embedding_path = join(root_path, 'embedding_data')\n",
        "reference_path = join(root_path, 'reference_data')\n",
        "evaluators_path = join(root_path, 'evaluators')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJUrQT_gZ5Z3"
      },
      "source": [
        "# Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V2oFYjlwHp1l"
      },
      "source": [
        "###Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "form",
        "id": "0RfuW_ILI5vx"
      },
      "outputs": [],
      "source": [
        "# @title Timestamp and Hashing Utilities\n",
        "def get_timestamp():\n",
        "  vietnam_timezone = pytz.timezone('Asia/Ho_Chi_Minh')\n",
        "  vietnam_time = datetime.datetime.now(vietnam_timezone)\n",
        "  return vietnam_time.isoformat()\n",
        "\n",
        "def md5(text):\n",
        "  '''MD% hash for track change'''\n",
        "  return hashlib.md5(text.encode('utf-8')).hexdigest()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "form",
        "id": "EwutUmxfMdsj"
      },
      "outputs": [],
      "source": [
        "# @title Pinecone Utilities\n",
        "def get_index(index_name=os.environ['PINECONE_INDEX_NAME']):\n",
        "  pc = Pinecone()\n",
        "  if not pc.has_index(index_name):\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1024,\n",
        "        metric='cosine',\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region='us-east-1'\n",
        "        )\n",
        "    )\n",
        "\n",
        "  return pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "cellView": "form",
        "id": "WTe9PQ6I8fV4"
      },
      "outputs": [],
      "source": [
        "# @title Document Loading and Processing Utilities\n",
        "def create_text_splitter(chunk_size=100, chunk_overlap=0):\n",
        "  return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "def Loader(file=None, mode='github', repo=config['repo'], branch=config['branch'], dest_dir='reference_data', format=('.txt', '.csv')):\n",
        "  if file is None and mode=='github':\n",
        "    loader = GithubFileLoader(\n",
        "      repo=repo,\n",
        "      branch=branch,\n",
        "      file_filter=lambda file_path: (file_path.endswith(format) and file_path.startswith(dest_dir))\n",
        "    )\n",
        "\n",
        "  if file is not None and mode=='local':\n",
        "    if file.endswith('.csv'):\n",
        "      loader = CSVLoader(file)\n",
        "    elif file.endswith('.txt'):\n",
        "      loader = TextLoader(file)\n",
        "\n",
        "  return loader\n",
        "\n",
        "def add_texts(text):\n",
        "  texts = text_splitter.split_text(text)\n",
        "  vectorstore.add_texts(texts)\n",
        "  print(f'Added {len(texts)} text chunks to vector store.')\n",
        "\n",
        "def add_documents(path=None, ids=None, mode='github'):\n",
        "  loader = Loader(file=path, mode=mode)\n",
        "  documents = loader.load()\n",
        "  src = '\\n'.join([doc.metadata['path'] for doc in documents]) if path is None else path\n",
        "\n",
        "  print(f'Loaded {len(documents)} document(s) from: {src}')\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "\n",
        "  for idx, each in enumerate(docs):\n",
        "    part = 0 if idx==0 or each.metadata['source']!=docs[idx-1].metadata['source'] else part + 1\n",
        "    each.metadata['timestamp'] = get_timestamp()\n",
        "    each.metadata['md5'] = md5(each.page_content)\n",
        "    if ids is None:\n",
        "      id = each.metadata['path'].split('/')[-1].split('.')[0] + f'_{part}'\n",
        "    else:\n",
        "      id = f'{ids}_{str(idx)}'\n",
        "    vectorstore.add_documents([each], ids=[id])\n",
        "  #vectorstore.add_documents(documents=docs)\n",
        "  print(f'Added {len(docs)} text chunk(s) to vector store.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "cellView": "form",
        "id": "SBX_Qx0pMcuq"
      },
      "outputs": [],
      "source": [
        "# @title LangSmith Dataset Utilities\n",
        "def create_dataset(file_name=None, dataset_name=None, provider='langsmith', mode='local', overwrite=False):\n",
        "  '''Load json file only'''\n",
        "  if mode=='local' and file_name!=None:\n",
        "    dataset_name = file_name[:-5] if dataset_name is None else dataset_name\n",
        "    path = join(embedding_path, provider, file_name)\n",
        "    with open(path) as f:\n",
        "      if not client.has_dataset(dataset_name=dataset_name):\n",
        "        client.create_dataset(dataset_name=dataset_name)\n",
        "        client.create_examples(\n",
        "          dataset_name=dataset_name,\n",
        "          examples=json.load(f)\n",
        "        )\n",
        "        print(f'Create {dataset_name} successfully from {path}!')\n",
        "      else:\n",
        "        print(f'Dataset {dataset_name} already exists.')\n",
        "\n",
        "  if mode=='github':\n",
        "    json_loader = Loader(mode='github', dest_dir=f'embedding_data/{provider}', format=('json'))\n",
        "    jsons = json_loader.load()\n",
        "    for each in jsons:\n",
        "      dataset_name = each.metadata['path'].split('/')[-1].split('.')[0]\n",
        "      if overwrite:\n",
        "        client.delete_dataset(dataset_name=dataset_name)\n",
        "      if not client.has_dataset(dataset_name=dataset_name):\n",
        "        client.create_dataset(dataset_name=dataset_name)\n",
        "        client.create_examples(\n",
        "          dataset_name=dataset_name,\n",
        "          examples=json.loads(each.page_content)\n",
        "        )\n",
        "        print(f'Create {dataset_name} successfully from {each.metadata[\"source\"]}!')\n",
        "      else:\n",
        "        print(f'Dataset {dataset_name} already exists.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EMr6Xf8SnsEt",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Evaluator Utilities\n",
        "def get_evaluator_path(evaluator_name, mode='github'):\n",
        "  if mode=='local':\n",
        "    path = join(evaluators_path, evaluator_name)\n",
        "  if mode=='github':\n",
        "    path = join('evaluators', evaluator_name)\n",
        "  return path\n",
        "\n",
        "def load_chat_prompt(evaluator, mode='github'):\n",
        "  role_map = {'system': SystemMessagePromptTemplate, 'user': HumanMessagePromptTemplate, 'ai': AIMessagePromptTemplate}\n",
        "  messages = []\n",
        "  file_path = join(get_evaluator_path(evaluator, mode), 'prompt.yaml')\n",
        "  if mode=='local':\n",
        "    with open(file_path, 'r') as f:\n",
        "      config = yaml.safe_load(f)\n",
        "  if mode=='github':\n",
        "    yaml_loader = Loader(mode=mode, dest_dir=file_path, format=('.yaml')).load()\n",
        "    config = yaml.safe_load(yaml_loader[0].page_content)\n",
        "\n",
        "  for msg_config in config['messages']:\n",
        "    messages.append(role_map[msg_config['role']].from_template(msg_config['template']))\n",
        "\n",
        "  return ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "def load_output_schema(evaluator, mode='github'):\n",
        "  file_path = join(get_evaluator_path(evaluator, mode), 'output_schema.yaml')\n",
        "  if mode=='local':\n",
        "    with open(file_path, 'r') as f:\n",
        "      config = yaml.safe_load(f)\n",
        "  if mode=='github':\n",
        "    yaml_loader = Loader(mode=mode, dest_dir=file_path, format=('.yaml')).load()\n",
        "    config = yaml.safe_load(yaml_loader[0].page_content)\n",
        "  return config\n",
        "\n",
        "def create_structured_grader_llm(evaluator=None):\n",
        "  return ChatLlamaCpp(\n",
        "    model_path=config['grader_model']['path'] if os.path.exists(config['grader_model']['path']) else config['grader_model']['alter_path'],\n",
        "    temperature=config['grader_model']['params']['temperature'],\n",
        "    n_ctx=config['grader_model']['params']['n_ctx'],\n",
        "    n_batch=config['grader_model']['params']['n_batch'],\n",
        "    max_tokens=config['grader_model']['params']['max_tokens'],\n",
        "    top_p=config['grader_model']['params']['top_p'],\n",
        "    top_k=config['grader_model']['params']['top_k'],\n",
        "    repeat_penalty=config['grader_model']['params']['repeat_penalty'],\n",
        "    stop=['\\n'],\n",
        "    grammar='''\n",
        "    start ::= object\n",
        "\n",
        "object ::= \"{\" ws\n",
        "              \"\\\"relevant\\\"\" ws \":\" ws boolean\n",
        "              (\",\" ws \"\\\"explanation\\\"\" ws \":\" ws string)?\n",
        "           ws \"}\"\n",
        "\n",
        "boolean ::= \"true\" | \"false\"\n",
        "\n",
        "string ::= ESCAPED_STRING\n",
        "\n",
        "# Standard JSON string (handles quotes inside correctly)\n",
        "ESCAPED_STRING ::= < \"\\\"\" ( [^\"\\\\] | \"\\\\\" [\"\\\\/bfnrt] | \"\\\\u\" [0-9a-fA-F]{4} )* \"\\\"\" >\n",
        "\n",
        "ws ::= < [ \\t\\n\\r]* >\n",
        "\n",
        "    ''',\n",
        "    streaming=False,\n",
        "    verbose=False,\n",
        "  ).with_structured_output(\n",
        "    load_output_schema(evaluator)\n",
        "  )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "cellView": "form",
        "id": "77983-fL1y0j"
      },
      "outputs": [],
      "source": [
        "# @title Tuning Ultilities\n",
        "def chunk_size_tunning(start=50, end=100, step=10, chunk_overlap=0, k=2, sync_embedded_data=False, mode='mini'):\n",
        "  for chunk_size in range(start, end+step, step):\n",
        "    namespace = f'chunk{chunk_size}_overlap{chunk_overlap}'\n",
        "    prefix = f'tune_{namespace}_k{k}'\n",
        "    print(f'Begin chunk tunning {prefix}')\n",
        "\n",
        "    init_global_var(namespace=namespace, chunk_size=chunk_size, chunk_overlap=chunk_overlap, k=k)\n",
        "\n",
        "    if sync_embedded_data:\n",
        "      add_documents(mode='github')\n",
        "\n",
        "    experiment_results = experiment(chunk_size=chunk_size, overlap=chunk_overlap, k=k, experiment_prefix=prefix, mode=mode)\n",
        "    pd_resutlts = experiment_results.to_pandas()\n",
        "    pd_resutlts['experiment'] = prefix\n",
        "    pd_resutlts['chunk'] = chunk_size\n",
        "    pd_resutlts['overlap'] = chunk_overlap\n",
        "    pd_resutlts['k'] = k\n",
        "    print(f'Complete chunk tunning {prefix}. TOTAL: {(chunk_size-start+step)/(end-start)*100}%')\n",
        "\n",
        "def overlap_tunning(best_chunk_size, start=10, end=30, step=10, k=2, sync_embedded_data=False, mode='mini'):\n",
        "  for overlap_ratio in range(start, end+step, step):\n",
        "    chunk_overlap = round(best_chunk_size*overlap_ratio/100)\n",
        "    namespace = f'chunk{best_chunk_size}_overlap{chunk_overlap}'\n",
        "    prefix = f'tune_{namespace}_k{k}'\n",
        "    print(f'Begin overlap tunning {prefix}')\n",
        "\n",
        "    init_global_var(namespace=namespace, chunk_size=best_chunk_size, chunk_overlap=chunk_overlap, k=k)\n",
        "\n",
        "    if sync_embedded_data:\n",
        "      add_documents(mode='github')\n",
        "\n",
        "    experiment_results = experiment(chunk_size=best_chunk_size, overlap=chunk_overlap, k=k, experiment_prefix=prefix, mode=mode)\n",
        "    pd_resutlts = experiment_results.to_pandas()\n",
        "    pd_resutlts['experiment'] = prefix\n",
        "    pd_resutlts['chunk'] = best_chunk_size\n",
        "    pd_resutlts['overlap'] = chunk_overlap\n",
        "    pd_resutlts['k'] = k\n",
        "    print(f'Complete overlap tunning {prefix}. TOTAL: {(overlap_ratio-start+step)/(end-start)*100}%')\n",
        "\n",
        "\n",
        "def k_tunning(best_chunk_size, best_overlap, start=2, end=7, step=1, sync_embedded_data=False, mode='mini'):\n",
        "  for k in range(start, end+step, step):\n",
        "    namespace = f'chunk{best_chunk_size}_overlap{best_overlap}'\n",
        "    prefix = f'tune_{namespace}_k{k}'\n",
        "    print(f'Begin k tunning {prefix}')\n",
        "\n",
        "    init_global_var(namespace=namespace, chunk_size=best_chunk_size, chunk_overlap=best_overlap, k=k)\n",
        "\n",
        "    if sync_embedded_data:\n",
        "      add_documents(mode='github')\n",
        "\n",
        "    experiment_results = experiment(chunk_size=best_chunk_size, overlap=best_overlap, k=k, experiment_prefix=prefix, mode=mode)\n",
        "    pd_resutlts = experiment_results.to_pandas()\n",
        "    pd_resutlts['experiment'] = prefix\n",
        "    pd_resutlts['chunk'] = best_chunk_size\n",
        "    pd_resutlts['overlap'] = best_overlap\n",
        "    pd_resutlts['k'] = k\n",
        "    print(f'Complete k tunning {prefix}. TOTAL: {(k-start+step)/(end-start)*100}%')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Deploy Utilities\n",
        "def display_local_tunnel_password():\n",
        "  password = !wget -q -O - https://loca.lt/mytunnelpassword\n",
        "  html = f\"\"\"\n",
        "  <button onclick=\"navigator.clipboard.writeText('{password[0]}').then(()=>{{this.innerText='Copied ✅';}}).catch(err=>{{alert('Copy failed: '+err);}})\">\n",
        "    Copy password\n",
        "  </button>\n",
        "  \"\"\"\n",
        "  display(HTML(html))\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "cellView": "form",
        "id": "Mkdl6ZPPoyBm"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Streamlit App\n",
        "\n",
        "%%writefile app.py\n",
        "import streamlit as st\n",
        "import os, json, time\n",
        "\n",
        "def show_chat_history():\n",
        "  if 'messages' not in st.session_state:\n",
        "    st.session_state.messages = st.session_state.messages = [{'role': 'assistant', 'content': 'Chào quý khách. Tôi có thể giúp được gì cho quý khách?'}]\n",
        "  for message in st.session_state.messages:\n",
        "    with st.chat_message(message['role']):\n",
        "      st.markdown(message['content'])\n",
        "\n",
        "def append_and_markdown(role, content):\n",
        "  with st.chat_message(role):\n",
        "    st.markdown(content)\n",
        "  st.session_state.messages.append({'role': role, 'content': content})\n",
        "  with open('chat_history.json', 'w') as f:\n",
        "    json.dump(st.session_state.messages, f)\n",
        "\n",
        "def reload(duration=3):\n",
        "  time.sleep(duration)\n",
        "  with open('chat_history.json') as f:\n",
        "    st.session_state.messages = json.load(f)\n",
        "\n",
        "def init_app(title='APChatbot', placeholder='Say something'):\n",
        "  st.title(title)\n",
        "  show_chat_history()\n",
        "  if user_input := st.chat_input(placeholder):\n",
        "    append_and_markdown('user', user_input)\n",
        "    append_and_markdown('assistant', f'Receive: {user_input}')\n",
        "    reload()\n",
        "\n",
        "init_app()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x_JEkLXPk79M",
        "outputId": "83aa7719-c7e6-4bda-b6d4-a1d62d29da4c"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting app.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "KCuTVFQOp-z_"
      },
      "outputs": [],
      "source": [
        "# @title Initialize Components\n",
        "def init_global_var(namespace=None, chunk_size=100, chunk_overlap=0, k=2):\n",
        "  global index, client, llm, embeddings, vectorstore, text_splitter, qa_chain\n",
        "\n",
        "  index = get_index()\n",
        "\n",
        "  client = Client()\n",
        "\n",
        "  #llm = ChatOpenAI(model=model)\n",
        "\n",
        "  llm = LlamaCpp(\n",
        "    model_path=config['chat_model']['path'] if os.path.exists(config['chat_model']['path']) else config['chat_model']['alter_path'],\n",
        "    temperature=config['chat_model']['params']['temperature'],\n",
        "    n_ctx=config['chat_model']['params']['n_ctx'],\n",
        "    n_batch=config['chat_model']['params']['n_batch'],\n",
        "    #suffix=config['chat_model']['params']['suffix'],\n",
        "    max_tokens=config['chat_model']['params']['max_tokens'],\n",
        "    top_p=config['chat_model']['params']['top_p'],\n",
        "    top_k=config['chat_model']['params']['top_k'],\n",
        "    repeat_penalty=config['chat_model']['params']['repeat_penalty'],\n",
        "    stop=['Assistant:', '\\n\\n'],\n",
        "    grammar='',\n",
        "    streaming=False,\n",
        "    verbose=False,\n",
        "  )\n",
        "\n",
        "  '''embeddings = HuggingFaceEndpointEmbeddings(model='Qwen/Qwen3-Embedding-8B')'''\n",
        "\n",
        "  embeddings = LlamaCppEmbeddings(\n",
        "    model_path=config['embedding_model']['path'] if os.path.exists(config['embedding_model']['path']) else config['embedding_model']['alter_path'],\n",
        "    n_ctx=config['embedding_model']['params']['n_ctx'],\n",
        "    n_batch=config['embedding_model']['params']['n_batch'],\n",
        "    verbose=False\n",
        "  )\n",
        "\n",
        "  vectorstore = PineconeVectorStore(index=index, embedding=embeddings, namespace=namespace)\n",
        "\n",
        "  text_splitter = create_text_splitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "  retriever = vectorstore.as_retriever(k=k, search_kwargs={ \"namespace\": namespace })\n",
        "\n",
        "  qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    combine_docs_chain_kwargs={'prompt': load_chat_prompt('qa_chain')},\n",
        "    return_source_documents=True\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "275hVmB1dD74"
      },
      "source": [
        "## Evaluators"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k61r41bTAaEX"
      },
      "source": [
        "### Evaluation Citeria"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "cellView": "form",
        "id": "vi1zoF_NfyRF"
      },
      "outputs": [],
      "source": [
        "# @title Correctness\n",
        "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    question=inputs['question'],\n",
        "    reference=reference_outputs['answer'],\n",
        "    answer=outputs['answer']\n",
        "  ))\n",
        "  return grade['correct']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "cellView": "form",
        "id": "LEHie9jeeQc9"
      },
      "outputs": [],
      "source": [
        "# @title Relevance\n",
        "def relevance(inputs: dict, outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    question=inputs['question'],\n",
        "    answer=outputs['answer']\n",
        "  ))\n",
        "  return grade['relevant']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "KpkhJOawfBrC",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Groundedness\n",
        "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    facts='\\n'.join(doc.page_content for doc in outputs['documents']),\n",
        "    question=inputs['question'],\n",
        "    answer=outputs['answer']\n",
        "  ))\n",
        "  return grade['grounded']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "50XKGfJpf3ew",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Retrieval relevance\n",
        "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    facts='\\n'.join(doc.page_content for doc in outputs['documents']),\n",
        "    question=inputs['question']\n",
        "  ))\n",
        "  return grade['relevant']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SJME-zuwAkx3"
      },
      "source": [
        "### Target and Experiment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "cellView": "form",
        "id": "46aanRpmFDYw"
      },
      "outputs": [],
      "source": [
        "# @title Target\n",
        "@traceable()\n",
        "def rag_bot(question: str) -> dict:\n",
        "  ai_msg = qa_chain.invoke({'question': question, 'chat_history': []})\n",
        "  return {'answer': ai_msg['answer'], 'documents': ai_msg['source_documents']}\n",
        "\n",
        "def target(inputs: dict) -> dict:\n",
        "  return rag_bot(inputs['question'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "Iqy-K-Qtwp3R",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Experiment\n",
        "def experiment(chunk_size, overlap, k, experiment_prefix='rag-doc-relevance', mode='mini', data='fqa_mini'):\n",
        "  if mode=='mini':\n",
        "    _data='fqa_mini'\n",
        "  if mode=='full':\n",
        "    _data='fqa_20250911'\n",
        "  else:\n",
        "    _data=data\n",
        "  experiment_results = client.evaluate(\n",
        "    target,\n",
        "    data=_data,\n",
        "    evaluators=[correctness, groundedness, relevance, retrieval_relevance],\n",
        "    experiment_prefix=experiment_prefix,\n",
        "    metadata=config\n",
        "  )\n",
        "\n",
        "  return experiment_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Run"
      ],
      "metadata": {
        "id": "isoSnWd-n0kE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#client.update_feedback(feedback_id='2e0e5ec1-d071-46f6-b0d8-a86e08d76201', correction='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "HaboJWTpFbZF",
        "outputId": "beaf5fc0-6252-4f90-ccda-3778e3dd4153",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LangSmithAPIError",
          "evalue": "Server error caused failure to PATCH /feedback/2e0e5ec1-d071-46f6-b0d8-a86e08d76201 in LangSmith API. HTTPError('500 Server Error: Internal Server Error for url: https://api.smith.langchain.com/feedback/2e0e5ec1-d071-46f6-b0d8-a86e08d76201', '{\"detail\":\"Internal server error\"}')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langsmith/utils.py\u001b[0m in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://api.smith.langchain.com/feedback/2e0e5ec1-d071-46f6-b0d8-a86e08d76201",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langsmith/client.py\u001b[0m in \u001b[0;36mrequest_with_retries\u001b[0;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m                         )\n\u001b[0;32m--> 828\u001b[0;31m                     \u001b[0mls_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status_with_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langsmith/utils.py\u001b[0m in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: [Errno 500 Server Error: Internal Server Error for url: https://api.smith.langchain.com/feedback/2e0e5ec1-d071-46f6-b0d8-a86e08d76201] {\"detail\":\"Internal server error\"}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLangSmithAPIError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_70/859435056.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_feedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeedback_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'2e0e5ec1-d071-46f6-b0d8-a86e08d76201'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrection\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langsmith/client.py\u001b[0m in \u001b[0;36mupdate_feedback\u001b[0;34m(self, feedback_id, score, value, correction, comment)\u001b[0m\n\u001b[1;32m   5993\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcomment\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5994\u001b[0m             \u001b[0mfeedback_update\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"comment\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcomment\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5995\u001b[0;31m         response = self.request_with_retries(\n\u001b[0m\u001b[1;32m   5996\u001b[0m             \u001b[0;34m\"PATCH\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5997\u001b[0m             \u001b[0;34mf\"/feedback/{_as_uuid(feedback_id, 'feedback_id')}\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langsmith/client.py\u001b[0m in \u001b[0;36mrequest_with_retries\u001b[0;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m                                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m                             raise ls_utils.LangSmithAPIError(\n\u001b[0m\u001b[1;32m    847\u001b[0m                                 \u001b[0;34mf\"Server error caused failure to {method}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m                                 \u001b[0;34mf\" {pathname} in\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLangSmithAPIError\u001b[0m: Server error caused failure to PATCH /feedback/2e0e5ec1-d071-46f6-b0d8-a86e08d76201 in LangSmith API. HTTPError('500 Server Error: Internal Server Error for url: https://api.smith.langchain.com/feedback/2e0e5ec1-d071-46f6-b0d8-a86e08d76201', '{\"detail\":\"Internal server error\"}')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kPioEazL4aZG"
      },
      "outputs": [],
      "source": [
        "#create_dataset(mode='github', overwrite=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DYuALVisoDlZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "201924ca36834dc7aeca003dea2df9b7",
            "0d8f0c170a2045d59a0a187baf418be9",
            "ff43d03092fb44cebbaec4d95d4bffe6",
            "75983e462a604eec8d57d173e6aa6762",
            "fc6d8de7e8fe4c06ae96361d39fc71f8",
            "a077cdc482dd496f93fee04cd8e9ea2a",
            "0baf8a4f5d1c4f569ae42d5f4834c5e1",
            "29394260778c458593d0d6c9cd27e245",
            "8696d1a5c3b74d059513e5099e346b30",
            "f8a3bb3c31f74988971f2b8376757c36",
            "a61565d0ce354ff583e8431209aca051",
            "e5a2474b923a495ba1d42efb4b514c7c",
            "42a0ad24a9a24039928d8f68f753787c",
            "03e0a61afcb946978683906b13c01842",
            "23a5db05433448bdb0b3d659cff5b853",
            "d84fa7d320a940df84468bf3f41aa22c",
            "e51bd54741664b40a31b56a7e9cbe9a2",
            "981e0f6b48e7497b87d1b96e035cb16b",
            "b6df01d0af2649e4a73cf15b249c19d5",
            "6c42d12683604c3ebefb19ee7eff712b",
            "1701d4ebe46644619b0f51904e16eb3c",
            "20a32c1083644371b494d00257f50c96"
          ]
        },
        "outputId": "9bf04372-0e40-4d61-a49c-4aa039cb7e3c",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin chunk tunning tune_chunk80_overlap0_k2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'tune_chunk80_overlap0_k2-d5b1971e' at:\n",
            "https://smith.langchain.com/o/00fa7863-e8c4-446a-9682-0dec08610f9c/datasets/4174ba46-6f89-4472-8832-1ab1e7684ea0/compare?selectedSessions=1e19ce16-cce7-47cf-80b7-92a6f8bb2c13\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "201924ca36834dc7aeca003dea2df9b7"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete chunk tunning tune_chunk80_overlap0_k2. TOTAL: 50.0%\n",
            "Begin chunk tunning tune_chunk90_overlap0_k2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'tune_chunk90_overlap0_k2-4fd0d886' at:\n",
            "https://smith.langchain.com/o/00fa7863-e8c4-446a-9682-0dec08610f9c/datasets/4174ba46-6f89-4472-8832-1ab1e7684ea0/compare?selectedSessions=0486d723-df7e-4977-8f7b-fe864d994055\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e5a2474b923a495ba1d42efb4b514c7c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ],
      "source": [
        "#chunk_size_tunning(mode='full')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "f5f81ccb4f3a49ae916fc704f2c781ed",
            "53a5574cf0704d06bc786739281fb68b",
            "12234c6507f9476491ce8055d2d998b3",
            "91129d8bfac44696b51299b5484cba32",
            "b6f80d1187024b618f9f3168fdcd0a4c",
            "4b4dec05ff8443928c534979f9b1fa35",
            "c652b01dae2a4a57ae80cd37c06ec536",
            "84cf031609f045a7bea5c6e33c059282",
            "6b54dd0d50e549be86a392d4829ab825",
            "09c0f71fb5ae4ad8bd9aac0d8efea4fd",
            "072845d8b4d84d939c283ddde8d91526",
            "ac18ed044fb644f6a35adf14239cf0de",
            "15a2f0e49cce4613b1c63aaf00f2e4ab",
            "55daca7d63f842ac88cec107a4bb41b1",
            "3e32a4527bd7472cb679b3e05a48e08b",
            "5dcc2ddee3f14fed9637570da31ab626",
            "19cffcff2b26402fae31ba2568893248",
            "90e296381c8c411b83a56dffc0d0ac3f",
            "43d7265e03e24af58de17d8246f81fc3",
            "78f483ad0be14d16b75fef8236657122",
            "07686e211db54f6db6b263f2d36be971",
            "13dab6d991d74e62806fc091affd332e",
            "b3a6a3eb640e4c02bad0b79f97420396",
            "9c73e893c89746db999ff7723e48b91e",
            "953ddbd98f684fccb8ea903ec5162463",
            "e1687a76d8784ee4b1ebc7ae66e8142f",
            "3726da38ecec4d2495484755b29d862a",
            "d7675ec8168648deb5b335dbcabcffb8",
            "41b67bb018d54ada97e6841f5e0d2639",
            "03feb93655014a8b8a04aaa0ba394a81",
            "4cc8f558f31a4942b28382a2433e10b4",
            "df6fae32e73f419cbc59d233bdcf98e9",
            "ff71fd376aaf42a69739badfdcb0d994",
            "66ef2e82c3854bee9b7da8529e627d73",
            "3d9f3cba40684c8faae35e1d3205ae59",
            "7d02c46b39b743fbba8b9ba670edc347",
            "041e5a79840049a6a7ec673cef2ed49c",
            "6e03e680fe7147829bfcb841fe638f31",
            "323790874f364d01a0ee55f2743cf2fd",
            "321a1a1423704b51b3d68e9ce29dc8a1",
            "b5d70bdd54ce418bb75140d54bcb68db",
            "cd9e6c28e49e4006ae5c6d0435ed42b6",
            "a2b61c08fafc4159821c1da9632f45d5",
            "c648d24d167e4e27b3641822da63642d"
          ]
        },
        "collapsed": true,
        "id": "GsYi9z3UeX9N",
        "outputId": "6b1b7ec5-6791-4ec1-bde8-6d76c3317f95"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin chunk tunning tune_chunk200_overlap0_k2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'tune_chunk200_overlap0_k2-03a1c0bf' at:\n",
            "https://smith.langchain.com/o/00fa7863-e8c4-446a-9682-0dec08610f9c/datasets/4174ba46-6f89-4472-8832-1ab1e7684ea0/compare?selectedSessions=6eb3de7b-6f72-46d5-9fa7-f7fc3ae40ff0\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f5f81ccb4f3a49ae916fc704f2c781ed"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete chunk tunning tune_chunk200_overlap0_k2. TOTAL: 33.33333333333333%\n",
            "Begin chunk tunning tune_chunk300_overlap0_k2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'tune_chunk300_overlap0_k2-d7a16f41' at:\n",
            "https://smith.langchain.com/o/00fa7863-e8c4-446a-9682-0dec08610f9c/datasets/4174ba46-6f89-4472-8832-1ab1e7684ea0/compare?selectedSessions=cc5c2528-b149-4605-b105-8b794f70d4f6\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ac18ed044fb644f6a35adf14239cf0de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete chunk tunning tune_chunk300_overlap0_k2. TOTAL: 66.66666666666666%\n",
            "Begin chunk tunning tune_chunk400_overlap0_k2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'tune_chunk400_overlap0_k2-f7898563' at:\n",
            "https://smith.langchain.com/o/00fa7863-e8c4-446a-9682-0dec08610f9c/datasets/4174ba46-6f89-4472-8832-1ab1e7684ea0/compare?selectedSessions=c57ab5c6-e1e7-488b-93ba-5e15af92b937\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b3a6a3eb640e4c02bad0b79f97420396"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete chunk tunning tune_chunk400_overlap0_k2. TOTAL: 100.0%\n",
            "Begin chunk tunning tune_chunk500_overlap0_k2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'tune_chunk500_overlap0_k2-886192ca' at:\n",
            "https://smith.langchain.com/o/00fa7863-e8c4-446a-9682-0dec08610f9c/datasets/4174ba46-6f89-4472-8832-1ab1e7684ea0/compare?selectedSessions=6dfe9883-2252-4cf7-b68d-8aedddc6119d\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "66ef2e82c3854bee9b7da8529e627d73"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        }
      ],
      "source": [
        "#chunk_size_tunning(start=200, end=500, step=100, mode='full')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#overlap_tunning(best_chunk_size=50, mode='full')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "155532bc48ca400eba17c5d7967f5ff8",
            "8f1bd242911347668b8798fa515b862b",
            "ae3d7795fc9749c7abe700ff71c85dfe",
            "d67af87d3a6d44249ad1d1fb3b100837",
            "4389453602f84d88bb799dddd27d0a12",
            "55cfb715fe5042319e76079cd24c0936",
            "50627e66f8ea4373bacb4afbcf554f7f",
            "83604135dcc543518a551d8ce0df6a3c",
            "c5eefdba355d4c5ea3611562ba55c303",
            "985f8a2282c1427badaffbcd909b3e13",
            "0a922f0771ed45adb46fe2a3c4129655",
            "35d28678110c4b4a9abe3f9fd99bd60d",
            "c6a8e9672073433da20109bc68495c84",
            "1ab49ede8e224715877ccbe16c6b7ffe",
            "761c90753b3d4b5b9ae9627e5b527f35",
            "b714ed6718b54bbba75ae2df55b4caa9",
            "22a61d4e56c24aef835d752700f278fb",
            "779356a2be6e4d1093faaf374b7d1668",
            "d743ae4e74504bd19ff8829803dea18d",
            "5da692f5150047118548da52e03ed121",
            "2da54db6f07646df9817319f7a724ba8",
            "69e9995ce5d14dc0882fab6de9b2cd08",
            "274014b90bdb4b48a967f40cece43e9b",
            "73c06fb711a14c7b8a78e23e58044816",
            "312cccfeff6741a4b63a093bf61d764d",
            "2fa02be491f2406392e5f3789b69df7a",
            "a5bfed7569864245aac0e8a9ec874a4d",
            "8f41eed41b1e485fbae87f29ea8db18c",
            "8cb684d2911e4d6485ad0db046bc7e57",
            "ddd2ee880aba44cb807b35e5e60776a9",
            "6d553466ac864116b131dcc805435808",
            "c2c8da355ffa478884a1b69f993155b3",
            "38aa29a3c2154e2ca4036926bb739a74"
          ]
        },
        "collapsed": true,
        "id": "hxq3VH45ICTQ",
        "outputId": "f0708cd3-ca4e-4e75-d8e8-853008d4bef0"
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin overlap tunning tune_chunk50_overlap5_k2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'tune_chunk50_overlap5_k2-b88364e1' at:\n",
            "https://smith.langchain.com/o/00fa7863-e8c4-446a-9682-0dec08610f9c/datasets/4174ba46-6f89-4472-8832-1ab1e7684ea0/compare?selectedSessions=044a4c57-3656-42f4-a872-fa0ecef6de0a\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "155532bc48ca400eba17c5d7967f5ff8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete overlap tunning tune_chunk50_overlap5_k2. TOTAL: 50.0%\n",
            "Begin overlap tunning tune_chunk50_overlap10_k2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'tune_chunk50_overlap10_k2-6ff6a911' at:\n",
            "https://smith.langchain.com/o/00fa7863-e8c4-446a-9682-0dec08610f9c/datasets/4174ba46-6f89-4472-8832-1ab1e7684ea0/compare?selectedSessions=669d98fa-b256-4a63-90ae-5c0ebe7f40ad\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35d28678110c4b4a9abe3f9fd99bd60d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete overlap tunning tune_chunk50_overlap10_k2. TOTAL: 100.0%\n",
            "Begin overlap tunning tune_chunk50_overlap15_k2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'tune_chunk50_overlap15_k2-357aae6b' at:\n",
            "https://smith.langchain.com/o/00fa7863-e8c4-446a-9682-0dec08610f9c/datasets/4174ba46-6f89-4472-8832-1ab1e7684ea0/compare?selectedSessions=b31d844f-b4b1-47bf-99f1-f56d71cf271f\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "274014b90bdb4b48a967f40cece43e9b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete overlap tunning tune_chunk50_overlap15_k2. TOTAL: 150.0%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import langsmith\n",
        "\n",
        "client = langsmith.Client()\n",
        "for e in client.list_feedback(run_ids='9035a001-39f7-4420-8fae-9e929afe026e'):\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 600
        },
        "id": "NuIXBQuFDbp0",
        "outputId": "b76d80b5-4545-498e-d5be-3604d69732ef",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "LangSmithAPIError",
          "evalue": "Server error caused failure to GET /feedback in LangSmith API. HTTPError('500 Server Error: Internal Server Error for url: https://api.smith.langchain.com/feedback?run=9035a001-39f7-4420-8fae-9e929afe026e&limit=100&offset=0', '{\"detail\":\"Internal server error\"}')",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langsmith/utils.py\u001b[0m in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    153\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m         \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/requests/models.py\u001b[0m in \u001b[0;36mraise_for_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1025\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhttp_error_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: 500 Server Error: Internal Server Error for url: https://api.smith.langchain.com/feedback?run=9035a001-39f7-4420-8fae-9e929afe026e&limit=100&offset=0",
            "\nThe above exception was the direct cause of the following exception:\n",
            "\u001b[0;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langsmith/client.py\u001b[0m in \u001b[0;36mrequest_with_retries\u001b[0;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[1;32m    827\u001b[0m                         )\n\u001b[0;32m--> 828\u001b[0;31m                     \u001b[0mls_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_for_status_with_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresponse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langsmith/utils.py\u001b[0m in \u001b[0;36mraise_for_status_with_text\u001b[0;34m(response)\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0me\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mhttpx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mHTTPStatusError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mHTTPError\u001b[0m: [Errno 500 Server Error: Internal Server Error for url: https://api.smith.langchain.com/feedback?run=9035a001-39f7-4420-8fae-9e929afe026e&limit=100&offset=0] {\"detail\":\"Internal server error\"}",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[0;31mLangSmithAPIError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipykernel_70/3418064698.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mclient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlangsmith\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mClient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0me\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mclient\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_feedback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'9035a001-39f7-4420-8fae-9e929afe026e'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langsmith/client.py\u001b[0m in \u001b[0;36mlist_feedback\u001b[0;34m(self, run_ids, feedback_key, feedback_source_type, limit, **kwargs)\u001b[0m\n\u001b[1;32m   6053\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfeedback_source_type\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6054\u001b[0m             \u001b[0mparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"source\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfeedback_source_type\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6055\u001b[0;31m         for i, feedback in enumerate(\n\u001b[0m\u001b[1;32m   6056\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_paginated_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/feedback\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6057\u001b[0m         ):\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langsmith/client.py\u001b[0m in \u001b[0;36m_get_paginated_list\u001b[0;34m(self, path, params)\u001b[0m\n\u001b[1;32m    982\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    983\u001b[0m             \u001b[0mparams_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"offset\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moffset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 984\u001b[0;31m             response = self.request_with_retries(\n\u001b[0m\u001b[1;32m    985\u001b[0m                 \u001b[0;34m\"GET\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    986\u001b[0m                 \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/langsmith/client.py\u001b[0m in \u001b[0;36mrequest_with_retries\u001b[0;34m(self, method, pathname, request_kwargs, stop_after_attempt, retry_on, to_ignore, handle_response, _context, **kwargs)\u001b[0m\n\u001b[1;32m    844\u001b[0m                                     \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    845\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 846\u001b[0;31m                             raise ls_utils.LangSmithAPIError(\n\u001b[0m\u001b[1;32m    847\u001b[0m                                 \u001b[0;34mf\"Server error caused failure to {method}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    848\u001b[0m                                 \u001b[0;34mf\" {pathname} in\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mLangSmithAPIError\u001b[0m: Server error caused failure to GET /feedback in LangSmith API. HTTPError('500 Server Error: Internal Server Error for url: https://api.smith.langchain.com/feedback?run=9035a001-39f7-4420-8fae-9e929afe026e&limit=100&offset=0', '{\"detail\":\"Internal server error\"}')"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#k_tunning(best_chunk_size=50, best_overlap=15, mode='full', start=6, end=7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 719,
          "referenced_widgets": [
            "15f482df2a314bfe9f0c02f57ae7b7d9",
            "8877d87cd4bd4de1ac354e51d54b3dd9",
            "58d948486de842d8aaeb26ce5ddd1f88",
            "e0a61f98af584fa3af208f7cfdb3fef5",
            "b580e8503cb5423a9acf150c54b13c37",
            "46ac87e7aada44b5b5959aa6fb52b310",
            "f81d3a3a190141d887dfcd94a2b661df",
            "f036efe9842b4519a113f4c7e601a83e",
            "6c93f8f7fa2346a09f599b59030db333",
            "38f0bdd0475e4854aa5d9d5a0f70fbc6",
            "7d694b8b06234f8eabcd8796f3181b40"
          ]
        },
        "collapsed": true,
        "id": "e513lLCBEr59",
        "outputId": "4c761d0b-5a79-4102-d144-395998ac6276"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Begin k tunning tune_chunk50_overlap15_k6\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "View the evaluation results for experiment: 'tune_chunk50_overlap15_k6-1ab0b5b7' at:\n",
            "https://smith.langchain.com/o/00fa7863-e8c4-446a-9682-0dec08610f9c/datasets/4174ba46-6f89-4472-8832-1ab1e7684ea0/compare?selectedSessions=0535b326-5152-451a-a062-f55707cb127f\n",
            "\n",
            "\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "0it [00:00, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15f482df2a314bfe9f0c02f57ae7b7d9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (2048) < n_ctx_train (131072) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hosting with localtunnel"
      ],
      "metadata": {
        "id": "aHN0_wAggOpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title Kill\n",
        "!pkill -f streamlit\n",
        "!pkill -f 'node /kaggle/working/node_modules/.bin/lt'\n",
        "!pkill -f localtunnel\n",
        "#!streamlit run app.py &>logs.txt &\n",
        "#!npx localtunnel --port 8501 --subdomain config['subdomain'] &>lt_url.txt &\n",
        "#Log process\n",
        "!ps -ef | grep node\n",
        "!ps -ef | grep streamlit\n",
        "!ps -ef | grep localtunnel\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUH0_Hlhmchw",
        "outputId": "ccf9e0bc-0189-4a73-eeaa-32c2cd8e76d5"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Stopping...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Terminated\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "root         801       1  0 Sep24 ?        00:00:03 [node] <defunct>\r\n",
            "root         870       1  0 Sep24 ?        00:00:05 [node] <defunct>\r\n",
            "root         898       1  0 Sep24 ?        00:00:05 [node] <defunct>\r\n",
            "root        1121       1  0 00:10 ?        00:00:03 [node] <defunct>\r\n",
            "root        1238       1  0 00:16 ?        00:00:02 [node] <defunct>\r\n",
            "root        1412       1  0 00:30 ?        00:00:01 [node] <defunct>\r\n",
            "root        2923      72  0 06:38 pts/0    00:00:00 /bin/bash -c ps -ef | grep node\r\n",
            "root        2925    2923  0 06:38 pts/0    00:00:00 grep node\n",
            "root         787       1  0 Sep24 ?        00:00:01 [streamlit] <defunct>\n",
            "root         855       1  0 Sep24 ?        00:00:01 [streamlit] <defunct>\n",
            "root         884       1  0 Sep24 ?        00:00:01 [streamlit] <defunct>\n",
            "root        1107       1  0 00:10 ?        00:00:00 [streamlit] <defunct>\n",
            "root        1222       1  0 00:16 ?        00:00:00 [streamlit] <defunct>\n",
            "root        1396       1  0 00:30 ?        00:00:01 [streamlit] <defunct>\n",
            "root        1479       1  0 00:41 ?        00:00:01 [streamlit] <defunct>\n",
            "root        1573       1  0 01:05 ?        00:00:01 [streamlit] <defunct>\n",
            "root        1703       1  0 01:12 ?        00:00:01 [streamlit] <defunct>\n",
            "root        1756       1  0 01:13 ?        00:00:01 [streamlit] <defunct>\n",
            "root        1852       1  0 01:18 ?        00:00:01 [streamlit] <defunct>\n",
            "root        1952       1  0 01:23 ?        00:00:01 [streamlit] <defunct>\n",
            "root        2003       1  0 01:24 ?        00:00:01 [streamlit] <defunct>\n",
            "root        2124       1  0 01:31 ?        00:00:00 [streamlit] <defunct>\n",
            "root        2170       1  0 01:32 ?        00:00:01 [streamlit] <defunct>\n",
            "root        2237       1  0 01:34 ?        00:00:01 [streamlit] <defunct>\n",
            "root        2287       1  0 01:35 ?        00:00:04 [streamlit] <defunct>\n",
            "root        2926      72  0 06:38 pts/0    00:00:00 /bin/bash -c ps -ef | grep streamlit\n",
            "root        2928    2926  0 06:38 pts/0    00:00:00 grep streamlit\n",
            "root        2929      72  0 06:38 pts/0    00:00:00 /bin/bash -c ps -ef | grep localtunnel\n",
            "root        2931    2929  0 06:38 pts/0    00:00:00 grep localtunnel\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import threading\n",
        "t1 = threading.Thread(target=os.system('streamlit run app.py &>logs.txt '))\n",
        "t2 = threading.Thread(target=os.system(f'npx localtunnel --port 8501 --subdomain {config[\"subdomain\"]} &>lt_url.txt'))"
      ],
      "metadata": {
        "id": "IlnP0OUVCd_d"
      },
      "execution_count": 122,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "display_local_tunnel_password()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 38
        },
        "id": "kqQINNh1HR7f",
        "outputId": "6e67b010-9da6-48b6-ffd3-055c3767a0cf"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "  <button onclick=\"navigator.clipboard.writeText('34.74.188.116').then(()=>{this.innerText='Copied ✅';}).catch(err=>{alert('Copy failed: '+err);})\">\n",
              "    Copy password\n",
              "  </button>\n",
              "  "
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 118,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vHYfqsHzq98N",
        "outputId": "999907ac-1047-4715-a05a-3cc05daff6f9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (40960) -- the full capacity of the model will not be utilized\n",
            "llama_context: n_ctx_per_seq (1024) < n_ctx_train (32768) -- the full capacity of the model will not be utilized\n",
            "init: embeddings required but some input tokens were not marked as outputs -> overriding\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'question': 'công ty thành lập năm nào',\n",
              " 'chat_history': [],\n",
              " 'answer': 'Công ty đã thành lập ngày 17/04/2018.\\nTrả lời: Công Ty Thành Lập Năm là [Năm]. \\nHiện tại chưa có dữ liệu. Quý khách vui lòng liên hệ bộ phận CSKH.\"',\n",
              " 'source_documents': [Document(id='landing_doanh_nghiep_0', metadata={'md5': '591288f7709d85d946c034938550cd80', 'path': 'reference_data/landing_doanh_nghiep.txt', 'sha': '0d767cb36634e666f1d93be75a0536b258224f2b', 'source': 'https://api.github.com/lehuong240823/rag-company-knowledge-consultant-chatbot/blob/main/reference_data/landing_doanh_nghiep.txt', 'timestamp': '2025-09-14T15:35:27.827657+07:00'}, page_content='# Giới thiệu công ty'),\n",
              "  Document(id='thong_tin_chi_nhanh_0', metadata={'md5': '4a39bebed256da8cf9c6cd268cc7d622', 'path': 'reference_data/thong_tin_chi_nhanh.csv', 'sha': '448be415b099a59b9ed7edd2aa2e767dd0a5581b', 'source': 'https://api.github.com/lehuong240823/rag-company-knowledge-consultant-chatbot/blob/main/reference_data/thong_tin_chi_nhanh.csv', 'timestamp': '2025-09-14T15:35:52.960535+07:00'}, page_content='\\ufeffID chi nhánh,Tên chi nhánh,Loại chi nhánh,Địa'),\n",
              "  Document(id='landing_doanh_nghiep_2', metadata={'md5': 'a021eb033b0238adc8280bf711a3a5ea', 'path': 'reference_data/landing_doanh_nghiep.txt', 'sha': '0d767cb36634e666f1d93be75a0536b258224f2b', 'source': 'https://api.github.com/lehuong240823/rag-company-knowledge-consultant-chatbot/blob/main/reference_data/landing_doanh_nghiep.txt', 'timestamp': '2025-09-14T15:35:29.068178+07:00'}, page_content='PHÁT, thành lập ngày 17/04/2018, MST: 2301025890,'),\n",
              "  Document(id='thong_tin_thue_20', metadata={'md5': 'c506b6dc4878535089ea2a9f15cf1d66', 'path': 'reference_data/thong_tin_thue.txt', 'sha': '2c4bd73a3461fd39283d6f1ee8cbed11a8d8d4e1', 'source': 'https://api.github.com/lehuong240823/rag-company-knowledge-consultant-chatbot/blob/main/reference_data/thong_tin_thue.txt', 'timestamp': '2025-09-14T15:36:36.038365+07:00'}, page_content='thuế hoặc Cổng thông tin doanh nghiệp quốc gia')]}"
            ]
          },
          "metadata": {},
          "execution_count": 118
        }
      ],
      "source": [
        "init_global_var(namespace='chunk50_overlap0', chunk_size=50, chunk_overlap=15, k=2)\n",
        "\n",
        "qa_chain.invoke({'question': 'công ty thành lập năm nào', 'chat_history': []})"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "V2oFYjlwHp1l",
        "275hVmB1dD74",
        "k61r41bTAaEX",
        "SJME-zuwAkx3"
      ],
      "generative_ai_disabled": true,
      "provenance": [],
      "mount_file_id": "1UvmP-pkUnlIOrt37odrdShOdS3-RVI7_",
      "authorship_tag": "ABX9TyPosiRXdsyZcs10IJ5NsrF8",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "201924ca36834dc7aeca003dea2df9b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0d8f0c170a2045d59a0a187baf418be9",
              "IPY_MODEL_ff43d03092fb44cebbaec4d95d4bffe6",
              "IPY_MODEL_75983e462a604eec8d57d173e6aa6762"
            ],
            "layout": "IPY_MODEL_fc6d8de7e8fe4c06ae96361d39fc71f8",
            "tabbable": null,
            "tooltip": null
          }
        },
        "0d8f0c170a2045d59a0a187baf418be9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_a077cdc482dd496f93fee04cd8e9ea2a",
            "placeholder": "​",
            "style": "IPY_MODEL_0baf8a4f5d1c4f569ae42d5f4834c5e1",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "ff43d03092fb44cebbaec4d95d4bffe6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_29394260778c458593d0d6c9cd27e245",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8696d1a5c3b74d059513e5099e346b30",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "75983e462a604eec8d57d173e6aa6762": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_f8a3bb3c31f74988971f2b8376757c36",
            "placeholder": "​",
            "style": "IPY_MODEL_a61565d0ce354ff583e8431209aca051",
            "tabbable": null,
            "tooltip": null,
            "value": " 20/? [18:05&lt;00:00, 52.79s/it]"
          }
        },
        "fc6d8de7e8fe4c06ae96361d39fc71f8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a077cdc482dd496f93fee04cd8e9ea2a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0baf8a4f5d1c4f569ae42d5f4834c5e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "29394260778c458593d0d6c9cd27e245": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "8696d1a5c3b74d059513e5099e346b30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f8a3bb3c31f74988971f2b8376757c36": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a61565d0ce354ff583e8431209aca051": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "e5a2474b923a495ba1d42efb4b514c7c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_42a0ad24a9a24039928d8f68f753787c",
              "IPY_MODEL_03e0a61afcb946978683906b13c01842",
              "IPY_MODEL_23a5db05433448bdb0b3d659cff5b853"
            ],
            "layout": "IPY_MODEL_d84fa7d320a940df84468bf3f41aa22c",
            "tabbable": null,
            "tooltip": null
          }
        },
        "42a0ad24a9a24039928d8f68f753787c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_e51bd54741664b40a31b56a7e9cbe9a2",
            "placeholder": "​",
            "style": "IPY_MODEL_981e0f6b48e7497b87d1b96e035cb16b",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "03e0a61afcb946978683906b13c01842": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_b6df01d0af2649e4a73cf15b249c19d5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c42d12683604c3ebefb19ee7eff712b",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "23a5db05433448bdb0b3d659cff5b853": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_1701d4ebe46644619b0f51904e16eb3c",
            "placeholder": "​",
            "style": "IPY_MODEL_20a32c1083644371b494d00257f50c96",
            "tabbable": null,
            "tooltip": null,
            "value": " 20/? [18:25&lt;00:00, 54.13s/it]"
          }
        },
        "d84fa7d320a940df84468bf3f41aa22c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e51bd54741664b40a31b56a7e9cbe9a2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "981e0f6b48e7497b87d1b96e035cb16b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "b6df01d0af2649e4a73cf15b249c19d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6c42d12683604c3ebefb19ee7eff712b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1701d4ebe46644619b0f51904e16eb3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20a32c1083644371b494d00257f50c96": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "f5f81ccb4f3a49ae916fc704f2c781ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_53a5574cf0704d06bc786739281fb68b",
              "IPY_MODEL_12234c6507f9476491ce8055d2d998b3",
              "IPY_MODEL_91129d8bfac44696b51299b5484cba32"
            ],
            "layout": "IPY_MODEL_b6f80d1187024b618f9f3168fdcd0a4c",
            "tabbable": null,
            "tooltip": null
          }
        },
        "53a5574cf0704d06bc786739281fb68b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_4b4dec05ff8443928c534979f9b1fa35",
            "placeholder": "​",
            "style": "IPY_MODEL_c652b01dae2a4a57ae80cd37c06ec536",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "12234c6507f9476491ce8055d2d998b3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_84cf031609f045a7bea5c6e33c059282",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6b54dd0d50e549be86a392d4829ab825",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "91129d8bfac44696b51299b5484cba32": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_09c0f71fb5ae4ad8bd9aac0d8efea4fd",
            "placeholder": "​",
            "style": "IPY_MODEL_072845d8b4d84d939c283ddde8d91526",
            "tabbable": null,
            "tooltip": null,
            "value": " 20/? [21:09&lt;00:00, 69.95s/it]"
          }
        },
        "b6f80d1187024b618f9f3168fdcd0a4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b4dec05ff8443928c534979f9b1fa35": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c652b01dae2a4a57ae80cd37c06ec536": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "84cf031609f045a7bea5c6e33c059282": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6b54dd0d50e549be86a392d4829ab825": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "09c0f71fb5ae4ad8bd9aac0d8efea4fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "072845d8b4d84d939c283ddde8d91526": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "ac18ed044fb644f6a35adf14239cf0de": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_15a2f0e49cce4613b1c63aaf00f2e4ab",
              "IPY_MODEL_55daca7d63f842ac88cec107a4bb41b1",
              "IPY_MODEL_3e32a4527bd7472cb679b3e05a48e08b"
            ],
            "layout": "IPY_MODEL_5dcc2ddee3f14fed9637570da31ab626",
            "tabbable": null,
            "tooltip": null
          }
        },
        "15a2f0e49cce4613b1c63aaf00f2e4ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_19cffcff2b26402fae31ba2568893248",
            "placeholder": "​",
            "style": "IPY_MODEL_90e296381c8c411b83a56dffc0d0ac3f",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "55daca7d63f842ac88cec107a4bb41b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_43d7265e03e24af58de17d8246f81fc3",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_78f483ad0be14d16b75fef8236657122",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "3e32a4527bd7472cb679b3e05a48e08b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_07686e211db54f6db6b263f2d36be971",
            "placeholder": "​",
            "style": "IPY_MODEL_13dab6d991d74e62806fc091affd332e",
            "tabbable": null,
            "tooltip": null,
            "value": " 20/? [23:27&lt;00:00, 70.92s/it]"
          }
        },
        "5dcc2ddee3f14fed9637570da31ab626": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "19cffcff2b26402fae31ba2568893248": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "90e296381c8c411b83a56dffc0d0ac3f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "43d7265e03e24af58de17d8246f81fc3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "78f483ad0be14d16b75fef8236657122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "07686e211db54f6db6b263f2d36be971": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "13dab6d991d74e62806fc091affd332e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "b3a6a3eb640e4c02bad0b79f97420396": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_9c73e893c89746db999ff7723e48b91e",
              "IPY_MODEL_953ddbd98f684fccb8ea903ec5162463",
              "IPY_MODEL_e1687a76d8784ee4b1ebc7ae66e8142f"
            ],
            "layout": "IPY_MODEL_3726da38ecec4d2495484755b29d862a",
            "tabbable": null,
            "tooltip": null
          }
        },
        "9c73e893c89746db999ff7723e48b91e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_d7675ec8168648deb5b335dbcabcffb8",
            "placeholder": "​",
            "style": "IPY_MODEL_41b67bb018d54ada97e6841f5e0d2639",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "953ddbd98f684fccb8ea903ec5162463": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_03feb93655014a8b8a04aaa0ba394a81",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4cc8f558f31a4942b28382a2433e10b4",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "e1687a76d8784ee4b1ebc7ae66e8142f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_df6fae32e73f419cbc59d233bdcf98e9",
            "placeholder": "​",
            "style": "IPY_MODEL_ff71fd376aaf42a69739badfdcb0d994",
            "tabbable": null,
            "tooltip": null,
            "value": " 20/? [25:15&lt;00:00, 75.85s/it]"
          }
        },
        "3726da38ecec4d2495484755b29d862a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d7675ec8168648deb5b335dbcabcffb8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "41b67bb018d54ada97e6841f5e0d2639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "03feb93655014a8b8a04aaa0ba394a81": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "4cc8f558f31a4942b28382a2433e10b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "df6fae32e73f419cbc59d233bdcf98e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ff71fd376aaf42a69739badfdcb0d994": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "66ef2e82c3854bee9b7da8529e627d73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3d9f3cba40684c8faae35e1d3205ae59",
              "IPY_MODEL_7d02c46b39b743fbba8b9ba670edc347",
              "IPY_MODEL_041e5a79840049a6a7ec673cef2ed49c"
            ],
            "layout": "IPY_MODEL_6e03e680fe7147829bfcb841fe638f31",
            "tabbable": null,
            "tooltip": null
          }
        },
        "3d9f3cba40684c8faae35e1d3205ae59": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_323790874f364d01a0ee55f2743cf2fd",
            "placeholder": "​",
            "style": "IPY_MODEL_321a1a1423704b51b3d68e9ce29dc8a1",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "7d02c46b39b743fbba8b9ba670edc347": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_b5d70bdd54ce418bb75140d54bcb68db",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cd9e6c28e49e4006ae5c6d0435ed42b6",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "041e5a79840049a6a7ec673cef2ed49c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_a2b61c08fafc4159821c1da9632f45d5",
            "placeholder": "​",
            "style": "IPY_MODEL_c648d24d167e4e27b3641822da63642d",
            "tabbable": null,
            "tooltip": null,
            "value": " 18/? [26:08&lt;00:00, 85.63s/it]"
          }
        },
        "6e03e680fe7147829bfcb841fe638f31": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "323790874f364d01a0ee55f2743cf2fd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "321a1a1423704b51b3d68e9ce29dc8a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "b5d70bdd54ce418bb75140d54bcb68db": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "cd9e6c28e49e4006ae5c6d0435ed42b6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a2b61c08fafc4159821c1da9632f45d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c648d24d167e4e27b3641822da63642d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "155532bc48ca400eba17c5d7967f5ff8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f1bd242911347668b8798fa515b862b",
              "IPY_MODEL_ae3d7795fc9749c7abe700ff71c85dfe",
              "IPY_MODEL_d67af87d3a6d44249ad1d1fb3b100837"
            ],
            "layout": "IPY_MODEL_4389453602f84d88bb799dddd27d0a12",
            "tabbable": null,
            "tooltip": null
          }
        },
        "8f1bd242911347668b8798fa515b862b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_55cfb715fe5042319e76079cd24c0936",
            "placeholder": "​",
            "style": "IPY_MODEL_50627e66f8ea4373bacb4afbcf554f7f",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "ae3d7795fc9749c7abe700ff71c85dfe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_83604135dcc543518a551d8ce0df6a3c",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c5eefdba355d4c5ea3611562ba55c303",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "d67af87d3a6d44249ad1d1fb3b100837": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_985f8a2282c1427badaffbcd909b3e13",
            "placeholder": "​",
            "style": "IPY_MODEL_0a922f0771ed45adb46fe2a3c4129655",
            "tabbable": null,
            "tooltip": null,
            "value": " 20/? [16:48&lt;00:00, 49.43s/it]"
          }
        },
        "4389453602f84d88bb799dddd27d0a12": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "55cfb715fe5042319e76079cd24c0936": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "50627e66f8ea4373bacb4afbcf554f7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "83604135dcc543518a551d8ce0df6a3c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "c5eefdba355d4c5ea3611562ba55c303": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "985f8a2282c1427badaffbcd909b3e13": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0a922f0771ed45adb46fe2a3c4129655": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "35d28678110c4b4a9abe3f9fd99bd60d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c6a8e9672073433da20109bc68495c84",
              "IPY_MODEL_1ab49ede8e224715877ccbe16c6b7ffe",
              "IPY_MODEL_761c90753b3d4b5b9ae9627e5b527f35"
            ],
            "layout": "IPY_MODEL_b714ed6718b54bbba75ae2df55b4caa9",
            "tabbable": null,
            "tooltip": null
          }
        },
        "c6a8e9672073433da20109bc68495c84": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_22a61d4e56c24aef835d752700f278fb",
            "placeholder": "​",
            "style": "IPY_MODEL_779356a2be6e4d1093faaf374b7d1668",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "1ab49ede8e224715877ccbe16c6b7ffe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_d743ae4e74504bd19ff8829803dea18d",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5da692f5150047118548da52e03ed121",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "761c90753b3d4b5b9ae9627e5b527f35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_2da54db6f07646df9817319f7a724ba8",
            "placeholder": "​",
            "style": "IPY_MODEL_69e9995ce5d14dc0882fab6de9b2cd08",
            "tabbable": null,
            "tooltip": null,
            "value": " 20/? [16:46&lt;00:00, 54.26s/it]"
          }
        },
        "b714ed6718b54bbba75ae2df55b4caa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "22a61d4e56c24aef835d752700f278fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "779356a2be6e4d1093faaf374b7d1668": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "d743ae4e74504bd19ff8829803dea18d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "5da692f5150047118548da52e03ed121": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2da54db6f07646df9817319f7a724ba8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "69e9995ce5d14dc0882fab6de9b2cd08": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "274014b90bdb4b48a967f40cece43e9b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_73c06fb711a14c7b8a78e23e58044816",
              "IPY_MODEL_312cccfeff6741a4b63a093bf61d764d",
              "IPY_MODEL_2fa02be491f2406392e5f3789b69df7a"
            ],
            "layout": "IPY_MODEL_a5bfed7569864245aac0e8a9ec874a4d",
            "tabbable": null,
            "tooltip": null
          }
        },
        "73c06fb711a14c7b8a78e23e58044816": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_8f41eed41b1e485fbae87f29ea8db18c",
            "placeholder": "​",
            "style": "IPY_MODEL_8cb684d2911e4d6485ad0db046bc7e57",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "312cccfeff6741a4b63a093bf61d764d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_ddd2ee880aba44cb807b35e5e60776a9",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6d553466ac864116b131dcc805435808",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "2fa02be491f2406392e5f3789b69df7a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_c2c8da355ffa478884a1b69f993155b3",
            "placeholder": "​",
            "style": "IPY_MODEL_38aa29a3c2154e2ca4036926bb739a74",
            "tabbable": null,
            "tooltip": null,
            "value": " 20/? [16:23&lt;00:00, 49.71s/it]"
          }
        },
        "a5bfed7569864245aac0e8a9ec874a4d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8f41eed41b1e485fbae87f29ea8db18c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8cb684d2911e4d6485ad0db046bc7e57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "ddd2ee880aba44cb807b35e5e60776a9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6d553466ac864116b131dcc805435808": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c2c8da355ffa478884a1b69f993155b3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "38aa29a3c2154e2ca4036926bb739a74": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "15f482df2a314bfe9f0c02f57ae7b7d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8877d87cd4bd4de1ac354e51d54b3dd9",
              "IPY_MODEL_58d948486de842d8aaeb26ce5ddd1f88",
              "IPY_MODEL_e0a61f98af584fa3af208f7cfdb3fef5"
            ],
            "layout": "IPY_MODEL_b580e8503cb5423a9acf150c54b13c37",
            "tabbable": null,
            "tooltip": null
          }
        },
        "8877d87cd4bd4de1ac354e51d54b3dd9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_46ac87e7aada44b5b5959aa6fb52b310",
            "placeholder": "​",
            "style": "IPY_MODEL_f81d3a3a190141d887dfcd94a2b661df",
            "tabbable": null,
            "tooltip": null,
            "value": ""
          }
        },
        "58d948486de842d8aaeb26ce5ddd1f88": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "ProgressView",
            "bar_style": "info",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_f036efe9842b4519a113f4c7e601a83e",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6c93f8f7fa2346a09f599b59030db333",
            "tabbable": null,
            "tooltip": null,
            "value": 1
          }
        },
        "e0a61f98af584fa3af208f7cfdb3fef5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "2.0.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "2.0.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_allow_html": false,
            "layout": "IPY_MODEL_38f0bdd0475e4854aa5d9d5a0f70fbc6",
            "placeholder": "​",
            "style": "IPY_MODEL_7d694b8b06234f8eabcd8796f3181b40",
            "tabbable": null,
            "tooltip": null,
            "value": " 6/? [04:44&lt;00:00, 46.35s/it]"
          }
        },
        "b580e8503cb5423a9acf150c54b13c37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "46ac87e7aada44b5b5959aa6fb52b310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f81d3a3a190141d887dfcd94a2b661df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        },
        "f036efe9842b4519a113f4c7e601a83e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "6c93f8f7fa2346a09f599b59030db333": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "38f0bdd0475e4854aa5d9d5a0f70fbc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "2.0.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border_bottom": null,
            "border_left": null,
            "border_right": null,
            "border_top": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7d694b8b06234f8eabcd8796f3181b40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLStyleModel",
          "model_module_version": "2.0.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "2.0.0",
            "_model_name": "HTMLStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "2.0.0",
            "_view_name": "StyleView",
            "background": null,
            "description_width": "",
            "font_size": null,
            "text_color": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}