{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lehuong240823/rag-company-knowledge-consultant-chatbot/blob/main/ChatBot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "20GrbDZgi1mj"
      },
      "source": [
        "# Get Started"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "QJQ8OZPL5war"
      },
      "outputs": [],
      "source": [
        "# @title Install Libraries\n",
        "!pip -q install pinecone llama-cpp-python\n",
        "!pip -q install langchain_community langchain_openai langchain_pinecone langchain-huggingface langsmith"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nRwdJs_82XKX",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Import Libraries\n",
        "import os, yaml, inspect, json, datetime, pytz, hashlib\n",
        "import pandas as pd\n",
        "from os.path import join\n",
        "from google.colab import userdata\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain_community.chat_models import ChatLlamaCpp\n",
        "from langchain.document_loaders import TextLoader, CSVLoader, GithubFileLoader\n",
        "from langchain.prompts import PromptTemplate, ChatPromptTemplate, SystemMessagePromptTemplate, HumanMessagePromptTemplate, AIMessagePromptTemplate\n",
        "from langchain.llms import LlamaCpp\n",
        "from langchain.embeddings import LlamaCppEmbeddings\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain_huggingface import HuggingFaceEndpointEmbeddings\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_pinecone.vectorstores import PineconeVectorStore\n",
        "from pinecone import Pinecone, ServerlessSpec\n",
        "from langsmith import Client, traceable"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Configuration\n",
        "config = yaml.safe_load('''\n",
        "repo: lehuong240823/rag-company-knowledge-consultant-chatbot\n",
        "branch: main\n",
        "ref_data: reference_data\n",
        "format:\n",
        "  - .txt\n",
        "  - .csv\n",
        "chat_model:\n",
        "  name: Qwen3-0.6B-Q8_0-GGUF\n",
        "  path: /content/Qwen3-0.6B-Q8_0.gguf\n",
        "  source: https://huggingface.co/Qwen/Qwen3-0.6B-GGUF/resolve/main/Qwen3-0.6B-Q8_0.gguf\n",
        "embedding_model:\n",
        "  name: Qwen3-Embedding-0.6B-Q8_0-GGUF\n",
        "  path: /content/Qwen3-Embedding-0.6B-Q8_0.gguf\n",
        "  source: https://huggingface.co/Qwen/Qwen3-Embedding-0.6B-GGUF/resolve/main/Qwen3-Embedding-0.6B-Q8_0.gguf\n",
        "grader_model:\n",
        "  name: 'Llama-3.2-1B-Instruct-Q8_0-GGUF'\n",
        "  path: '/content/llama-3.2-1b-instruct-q8_0.gguf'\n",
        "  source: 'https://huggingface.co/hugging-quants/Llama-3.2-1B-Instruct-Q8_0-GGUF/resolve/main/llama-3.2-1b-instruct-q8_0.gguf'\n",
        "\n",
        "''')"
      ],
      "metadata": {
        "id": "utocHmVI51nB",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Download Quantized Model\n",
        "!wget -N -q {config['chat_model']['source']}\n",
        "!wget -N -q {config['embedding_model']['source']}\n",
        "!wget -N -q {config['grader_model']['source']}"
      ],
      "metadata": {
        "collapsed": true,
        "id": "gGF99SqXsnpF",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SC-bUdQ15Aq",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Set Environment Variables\n",
        "os.environ['HUGGINGFACEHUB_API_TOKEN'] = userdata.get('HF_TOKEN')\n",
        "os.environ['LANGSMITH_API_KEY'] = userdata.get('LANGSMITH_API_KEY')\n",
        "os.environ['LANGSMITH_ENDPOINT'] = 'https://api.smith.langchain.com'\n",
        "os.environ['LANGSMITH_PROJECT'] = userdata.get('PINECONE_INDEX_NAME')\n",
        "os.environ['LANGSMITH_TRACING'] = 'true'\n",
        "os.environ['OPENAI_API_BASE'] = 'https://router.huggingface.co/v1' #'https://openrouter.ai/api/v1'\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('HF_TOKEN') #userdata.get('OPENROUTER_API_KEY')\n",
        "os.environ['PINECONE_API_KEY'] = userdata.get('PINECONE_API_KEY')\n",
        "os.environ['PINECONE_INDEX_NAME'] = userdata.get('PINECONE_INDEX_NAME')\n",
        "os.environ['GITHUB_PERSONAL_ACCESS_TOKEN'] = userdata.get('GITHUB_PERSONAL_ACCESS_TOKEN')\n",
        "root_path =  userdata.get('ROOT_PATH')\n",
        "embedding_path = join(root_path, 'embedding_data')\n",
        "reference_path = join(root_path, 'reference_data')\n",
        "evaluators_path = join(root_path, 'evaluators')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJUrQT_gZ5Z3"
      },
      "source": [
        "# Method"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Utility Functions"
      ],
      "metadata": {
        "id": "V2oFYjlwHp1l"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0RfuW_ILI5vx",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Timestamp and Hashing Utilities\n",
        "def get_timestamp():\n",
        "  vietnam_timezone = pytz.timezone('Asia/Ho_Chi_Minh')\n",
        "  vietnam_time = datetime.datetime.now(vietnam_timezone)\n",
        "  return vietnam_time.isoformat()\n",
        "\n",
        "def md5(text):\n",
        "  '''MD% hash for track change'''\n",
        "  return hashlib.md5(text.encode('utf-8')).hexdigest()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EwutUmxfMdsj"
      },
      "outputs": [],
      "source": [
        "# @title Pinecone Utilities\n",
        "def get_index(index_name=os.environ['PINECONE_INDEX_NAME']):\n",
        "  pc = Pinecone()\n",
        "  if not pc.has_index(index_name):\n",
        "    pc.create_index(\n",
        "        name=index_name,\n",
        "        dimension=1024,\n",
        "        metric='cosine',\n",
        "        spec=ServerlessSpec(\n",
        "            cloud='aws',\n",
        "            region='us-east-1'\n",
        "        )\n",
        "    )\n",
        "\n",
        "  return pc.Index(index_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WTe9PQ6I8fV4",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Document Loading and Processing Utilities\n",
        "def create_text_splitter(chunk_size=100, chunk_overlap=0):\n",
        "  return RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "def Loader(file=None, mode='github', repo=config['repo'], branch=config['branch'], dest_dir='reference_data', format=('.txt', '.csv')):\n",
        "  if file is None and mode=='github':\n",
        "    loader = GithubFileLoader(\n",
        "      repo=repo,\n",
        "      branch=branch,\n",
        "      file_filter=lambda file_path: (file_path.endswith(format) and file_path.startswith(dest_dir))\n",
        "    )\n",
        "\n",
        "  if file is not None and mode=='local':\n",
        "    if file.endswith('.csv'):\n",
        "      loader = CSVLoader(file)\n",
        "    elif file.endswith('.txt'):\n",
        "      loader = TextLoader(file)\n",
        "\n",
        "  return loader\n",
        "\n",
        "def add_texts(text):\n",
        "  texts = text_splitter.split_text(text)\n",
        "  vectorstore.add_texts(texts)\n",
        "  print(f'Added {len(texts)} text chunks to vector store.')\n",
        "\n",
        "def add_documents(path=None, ids=None, mode='github'):\n",
        "  loader = Loader(file=path, mode=mode)\n",
        "  documents = loader.load()\n",
        "  src = '\\n'.join([doc.metadata['path'] for doc in documents]) if path is None else path\n",
        "\n",
        "  print(f'Loaded {len(documents)} document(s) from: {src}')\n",
        "  docs = text_splitter.split_documents(documents)\n",
        "\n",
        "  for idx, each in enumerate(docs):\n",
        "    part = 0 if idx==0 or each.metadata['source']!=docs[idx-1].metadata['source'] else part + 1\n",
        "    each.metadata['timestamp'] = get_timestamp()\n",
        "    each.metadata['md5'] = md5(each.page_content)\n",
        "    if ids is None:\n",
        "      id = each.metadata['path'].split('/')[-1].split('.')[0] + f'_{part}'\n",
        "    else:\n",
        "      id = f'{ids}_{str(idx)}'\n",
        "    vectorstore.add_documents([each], ids=[id])\n",
        "  #vectorstore.add_documents(documents=docs)\n",
        "  print(f'Added {len(docs)} text chunk(s) to vector store.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "SBX_Qx0pMcuq"
      },
      "outputs": [],
      "source": [
        "# @title LangSmith Dataset Utilities\n",
        "def create_dataset(file_name=None, dataset_name=None, provider='langsmith', mode='local', overwrite=False):\n",
        "  '''Load json file only'''\n",
        "  if mode=='local' and file_name!=None:\n",
        "    dataset_name = file_name[:-5] if dataset_name is None else dataset_name\n",
        "    path = join(embedding_path, provider, file_name)\n",
        "    with open(path) as f:\n",
        "      if not client.has_dataset(dataset_name=dataset_name):\n",
        "        client.create_dataset(dataset_name=dataset_name)\n",
        "        client.create_examples(\n",
        "          dataset_name=dataset_name,\n",
        "          examples=json.load(f)\n",
        "        )\n",
        "        print(f'Create {dataset_name} successfully from {path}!')\n",
        "      else:\n",
        "        print(f'Dataset {dataset_name} already exists.')\n",
        "\n",
        "  if mode=='github':\n",
        "    json_loader = Loader(mode='github', dest_dir=f'embedding_data/{provider}', format=('json'))\n",
        "    jsons = json_loader.load()\n",
        "    for each in jsons:\n",
        "      dataset_name = each.metadata['path'].split('/')[-1].split('.')[0]\n",
        "      if overwrite:\n",
        "        client.delete_dataset(dataset_name=dataset_name)\n",
        "      if not client.has_dataset(dataset_name=dataset_name):\n",
        "        client.create_dataset(dataset_name=dataset_name)\n",
        "        client.create_examples(\n",
        "          dataset_name=dataset_name,\n",
        "          examples=json.loads(each.page_content)\n",
        "        )\n",
        "        print(f'Create {dataset_name} successfully from {each.metadata[\"source\"]}!')\n",
        "      else:\n",
        "        print(f'Dataset {dataset_name} already exists.')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMr6Xf8SnsEt"
      },
      "outputs": [],
      "source": [
        "# @title Evaluator Utilities\n",
        "def get_evaluator_path(evaluator_name, mode='github'):\n",
        "  if mode=='local':\n",
        "    path = join(evaluators_path, evaluator_name)\n",
        "  if mode=='github':\n",
        "    path = join('evaluators', evaluator_name)\n",
        "  return path\n",
        "\n",
        "def load_chat_prompt(evaluator, mode='github'):\n",
        "  role_map = {'system': SystemMessagePromptTemplate, 'user': HumanMessagePromptTemplate, 'ai': AIMessagePromptTemplate}\n",
        "  messages = []\n",
        "  file_path = join(get_evaluator_path(evaluator, mode), 'prompt.yaml')\n",
        "  if mode=='local':\n",
        "    with open(file_path, 'r') as f:\n",
        "      config = yaml.safe_load(f)\n",
        "  if mode=='github':\n",
        "    yaml_loader = Loader(mode=mode, dest_dir=file_path, format=('.yaml')).load()\n",
        "    config = yaml.safe_load(yaml_loader[0].page_content)\n",
        "\n",
        "  for msg_config in config['messages']:\n",
        "    messages.append(role_map[msg_config['role']].from_template(msg_config['template']))\n",
        "\n",
        "  return ChatPromptTemplate.from_messages(messages)\n",
        "\n",
        "def load_output_schema(evaluator, mode='github'):\n",
        "  file_path = join(get_evaluator_path(evaluator, mode), 'output_schema.yaml')\n",
        "  if mode=='local':\n",
        "    with open(file_path, 'r') as f:\n",
        "      config = yaml.safe_load(f)\n",
        "  if mode=='github':\n",
        "    yaml_loader = Loader(mode=mode, dest_dir=file_path, format=('.yaml')).load()\n",
        "    config = yaml.safe_load(yaml_loader[0].page_content)\n",
        "  return config\n",
        "\n",
        "def create_structured_grader_llm(temperature=0, evaluator=None):\n",
        "  return ChatLlamaCpp(\n",
        "    temperature=0,\n",
        "    model_path='/content/llama-3.2-1b-instruct-q8_0.gguf',\n",
        "    n_ctx=2048,\n",
        "    n_batch=256,\n",
        "    max_tokens=200,\n",
        "    top_p=0.8,\n",
        "    top_k=20,\n",
        "    repeat_penalty=1.5,\n",
        "    verbose=False,\n",
        "    streaming=False,\n",
        "  ).with_structured_output(\n",
        "    load_output_schema(evaluator),\n",
        "  )\n",
        "  \"\"\"return ChatOpenAI(model=model, temperature=temperature).with_structured_output(\n",
        "    load_output_schema(evaluator),\n",
        "    method='json_schema', strict=True\n",
        "  )\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "275hVmB1dD74"
      },
      "source": [
        "## Evaluators"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation Citeria"
      ],
      "metadata": {
        "id": "k61r41bTAaEX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vi1zoF_NfyRF",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Correctness\n",
        "def correctness(inputs: dict, outputs: dict, reference_outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    question=inputs['question'],\n",
        "    reference=reference_outputs['answer'],\n",
        "    answer=outputs['answer']\n",
        "  ))\n",
        "  return grade['correct']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "LEHie9jeeQc9"
      },
      "outputs": [],
      "source": [
        "# @title Relevance\n",
        "def relevance(inputs: dict, outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    question=inputs['question'],\n",
        "    answer=outputs['answer']\n",
        "  ))\n",
        "  return grade['relevant']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "KpkhJOawfBrC"
      },
      "outputs": [],
      "source": [
        "# @title Groundedness\n",
        "def groundedness(inputs: dict, outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    facts='\\n'.join(doc.page_content for doc in outputs['documents']),\n",
        "    answer=outputs['answer']\n",
        "  ))\n",
        "  return grade['grounded']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "50XKGfJpf3ew"
      },
      "outputs": [],
      "source": [
        "# @title Retrieval relevance\n",
        "def retrieval_relevance(inputs: dict, outputs: dict) -> bool:\n",
        "  evaluator = inspect.currentframe().f_code.co_name\n",
        "  grader_llm = create_structured_grader_llm(evaluator=evaluator)\n",
        "  grade = grader_llm.invoke(load_chat_prompt(evaluator).format(\n",
        "    facts='\\n'.join(doc.page_content for doc in outputs['documents']),\n",
        "    question=inputs['question']\n",
        "  ))\n",
        "  return grade['relevant']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Target and Experiment"
      ],
      "metadata": {
        "id": "SJME-zuwAkx3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "46aanRpmFDYw"
      },
      "outputs": [],
      "source": [
        "# @title Target\n",
        "@traceable()\n",
        "def rag_bot(question: str) -> dict:\n",
        "  ai_msg = qa_chain.invoke({'question': question, 'chat_history': []})\n",
        "  return {'answer': ai_msg['answer'], 'documents': ai_msg['source_documents']}\n",
        "\n",
        "def target(inputs: dict) -> dict:\n",
        "  return rag_bot(inputs['question'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Iqy-K-Qtwp3R",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Experiment\n",
        "def experiment(chunk_size, overlap, k, experiment_prefix='rag-doc-relevance', data='fqa_mini'):\n",
        "  experiment_results = client.evaluate(\n",
        "    target,\n",
        "    data=data,\n",
        "    evaluators=[correctness, relevance, groundedness, retrieval_relevance],\n",
        "    experiment_prefix=experiment_prefix\n",
        "  )\n",
        "\n",
        "  return experiment_results"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tuning"
      ],
      "metadata": {
        "id": "nJ3LO6Jw9kGa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Dump Experiments to CSV\n",
        "def save_experiment(df, path=join(root_path, 'experiments', 'tuning_experiments')):\n",
        "  if not os.path.isfile(path):\n",
        "    return df.to_csv(path, index=False, mode='w', encoding='utf-8')\n",
        "  else:\n",
        "    return df.to_csv(path, index=False, mode='a', header=False, encoding='utf-8')\n",
        "  print(f'Save {path} successfully!')"
      ],
      "metadata": {
        "cellView": "form",
        "id": "jARHsIIwQjAc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# @title Chunk Size\n",
        "def chunk_size_tunning(start=200, end=500, step=100, chunk_overlap=0, k=2, sync_embedded_data=False):\n",
        "  for chunk_size in range(start, end+step, step):\n",
        "    namespace = f'chunk{chunk_size}_overlap{chunk_overlap}'\n",
        "    prefix = f'tune_{namespace}_k{k}'\n",
        "    print(f'Begin chunk tunning {prefix}')\n",
        "\n",
        "    init_global_var(namespace=namespace, chunk_size=chunk_size, chunk_overlap=chunk_overlap, k=k)\n",
        "\n",
        "    if sync_embedded_data:\n",
        "      add_documents(mode='github')\n",
        "\n",
        "    experiment_results = experiment(chunk_size=chunk_size, overlap=chunk_overlap, k=k, experiment_prefix=prefix)\n",
        "    pd_resutlts = experiment_results.to_pandas()\n",
        "    pd_resutlts['experiment'] = prefix\n",
        "    pd_resutlts['chunk'] = chunk_size\n",
        "    pd_resutlts['overlap'] = chunk_overlap\n",
        "    pd_resutlts['k'] = k\n",
        "\n",
        "    print(f'Complete chunk tunning {prefix}. TOTAL: {(chunk_size-start)/(end-start+step)*100}%')"
      ],
      "metadata": {
        "id": "77983-fL1y0j",
        "cellView": "form"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "KCuTVFQOp-z_",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Initialize Components\n",
        "def init_global_var(namespace=None, chunk_size=100, chunk_overlap=0, k=2, model='Qwen/Qwen3-8B:featherless-ai'):\n",
        "  global index, client, llm, embeddings, vectorstore, text_splitter, qa_chain\n",
        "\n",
        "  index = get_index()\n",
        "\n",
        "  client = Client()\n",
        "\n",
        "  #llm = ChatOpenAI(model=model)\n",
        "\n",
        "  llm = LlamaCpp(\n",
        "    model_path=config['chat_model']['path'],\n",
        "    temperature=0,\n",
        "    n_ctx=1024,\n",
        "    max_tokens=100,\n",
        "    n_batch=256,\n",
        "    top_p=0.8,\n",
        "    top_k=20,\n",
        "    repeat_penalty=1.5,\n",
        "    stop=[\"\\n\"],\n",
        "    streaming=False,\n",
        "    verbose=False,\n",
        "  )\n",
        "\n",
        "  '''embeddings = HuggingFaceEndpointEmbeddings(model='Qwen/Qwen3-Embedding-8B')'''\n",
        "\n",
        "  embeddings = LlamaCppEmbeddings(\n",
        "    model_path=config['embedding_model']['path'],\n",
        "    verbose=False\n",
        "  )\n",
        "\n",
        "  vectorstore = PineconeVectorStore(index=index, embedding=embeddings, namespace=namespace)\n",
        "\n",
        "  text_splitter = create_text_splitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
        "\n",
        "  retriever = vectorstore.as_retriever(k=k, search_kwargs={ \"namespace\": namespace })\n",
        "\n",
        "  qa_chain = ConversationalRetrievalChain.from_llm(\n",
        "    llm,\n",
        "    retriever=retriever,\n",
        "    combine_docs_chain_kwargs={'prompt': load_chat_prompt('qa_chain')},\n",
        "    return_source_documents=True\n",
        "  )\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "init_global_var(namespace='chunk200_overlap0', )\n",
        "experiment_results = experiment(chunk_size=100, overlap=0, k=2, experiment_prefix='TEST')"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XxG49-1Rflyk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "create_dataset(mode='github', overwrite=False)"
      ],
      "metadata": {
        "id": "kPioEazL4aZG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GsYi9z3UeX9N",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "chunk_size_tunning()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4YgM_RtXZXIn"
      },
      "outputs": [],
      "source": [
        "print(experiment_results.to_pandas().to_markdown())"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "20GrbDZgi1mj",
        "k61r41bTAaEX",
        "SJME-zuwAkx3"
      ],
      "runtime_attributes": {
        "runtime_version": "2025.07"
      },
      "generative_ai_disabled": true,
      "mount_file_id": "1UvmP-pkUnlIOrt37odrdShOdS3-RVI7_",
      "authorship_tag": "ABX9TyP88IW+Kyw6Ky37bsQinZUO",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}